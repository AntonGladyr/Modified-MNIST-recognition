{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_88.62_Anton",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonGladyr/COMP551-MiniProject1/blob/master/CNN_88.62_Anton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nJ47ABzStdv",
        "colab_type": "code",
        "outputId": "752d60eb-8889-4526-9434-605b682a785a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hWJmOgwzCWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Net, self).__init__()\n",
        "        # Defining a 2D convolution layer\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1),\n",
        "            # nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            # nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            # nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "        self.drop_out = nn.Dropout()\n",
        "\n",
        "        # Defining another 2D convolution layer\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(6*6*128, 512)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-TGbFWTQ49",
        "colab_type": "code",
        "outputId": "5ab79126-8267-470d-dc79-9364fbfeec33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from torch import nn, optim\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# number of times we iterate over the training set\n",
        "EPOCH = 70\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 512\n",
        "TRAIN_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_x'\n",
        "TEST_PATH = ''\n",
        "TARGETS_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_y.csv'\n",
        "THRESH = 240\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('torch.cuda.device_count(): {0}'.format(torch.cuda.device_count()))\n",
        "    print('torch.cuda.get_device_name: {0}'.format(torch.cuda.get_device_name(0)))\n",
        "    # load images as a numpy array\n",
        "    train_dataset = np.array(np.load(TRAIN_PATH, allow_pickle=True))\n",
        "    # train_dataset = np.array([cv2.threshold(i, THRESH, 255, cv2.THRESH_BINARY)[1] for i in train_dataset])\n",
        "    train_dataset = train_dataset / 255.0\n",
        "\n",
        "    targets = pd.read_csv(TARGETS_PATH, delimiter=',', skipinitialspace=True)\n",
        "    targets = targets.to_numpy()\n",
        "    # remove id column\n",
        "    targets = targets[:, 1]\n",
        "    targets = targets.astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(train_dataset, targets, test_size=0.2, random_state=42)\n",
        "    # Clean memory\n",
        "    len_train_dataset = len(X_train)\n",
        "    train_dataset = None\n",
        "    targets = None\n",
        "\n",
        "    X_train = torch.from_numpy(X_train)\n",
        "    y_train = torch.from_numpy(y_train)\n",
        "    X_test = torch.from_numpy(X_test)\n",
        "    y_test = torch.from_numpy(y_test)\n",
        "    y_train = y_train.long()\n",
        "    y_test = y_test.long()\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test,y_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "\n",
        "    # defining the model\n",
        "    model = Net().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    time0 = time()\n",
        "    epochs = EPOCH\n",
        "\n",
        "    for e in range(epochs):\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # clearing the Gradients of the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prediction for training\n",
        "        output_train = model(inputs)\n",
        "        \n",
        "        # computing the training loss\n",
        "        loss_train = criterion(output_train, labels)\n",
        "\n",
        "        # computing the updated weights of all the model parameters\n",
        "        loss_train.backward()\n",
        "\n",
        "        # And optimizes its weights here\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_train.item()\n",
        "      else:\n",
        "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss / len_train_dataset))\n",
        "\n",
        "    print(\"\\nTraining Time (in minutes) =\", (time() - time0) / 60)\n",
        "\n",
        "    # prediction for validation set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            output = model(images)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # accuracy on validation set\n",
        "    print(\"\\nModel Accuracy =\", (100 * correct / total))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.cuda.device_count(): 1\n",
            "torch.cuda.get_device_name: Tesla K80\n",
            "Epoch 0 - Training loss: 0.003794303447008133\n",
            "Epoch 1 - Training loss: 0.0037318435847759246\n",
            "Epoch 2 - Training loss: 0.0037289428770542147\n",
            "Epoch 3 - Training loss: 0.0037311215877532957\n",
            "Epoch 4 - Training loss: 0.0037207226902246475\n",
            "Epoch 5 - Training loss: 0.0035618900954723356\n",
            "Epoch 6 - Training loss: 0.003032822808623314\n",
            "Epoch 7 - Training loss: 0.002318021932244301\n",
            "Epoch 8 - Training loss: 0.0018472889363765716\n",
            "Epoch 9 - Training loss: 0.0015236009925603867\n",
            "Epoch 10 - Training loss: 0.001299541225284338\n",
            "Epoch 11 - Training loss: 0.001133014465868473\n",
            "Epoch 12 - Training loss: 0.0010106897361576556\n",
            "Epoch 13 - Training loss: 0.0009234689556062222\n",
            "Epoch 14 - Training loss: 0.0008492732871323824\n",
            "Epoch 15 - Training loss: 0.0007921120941638947\n",
            "Epoch 16 - Training loss: 0.000742706960439682\n",
            "Epoch 17 - Training loss: 0.000692694691196084\n",
            "Epoch 18 - Training loss: 0.0006981262169778347\n",
            "Epoch 19 - Training loss: 0.0006389618752524257\n",
            "Epoch 20 - Training loss: 0.0006112039286643267\n",
            "Epoch 21 - Training loss: 0.0005950328737497329\n",
            "Epoch 22 - Training loss: 0.0005508680071681738\n",
            "Epoch 23 - Training loss: 0.000523035900760442\n",
            "Epoch 24 - Training loss: 0.0005276819071732461\n",
            "Epoch 25 - Training loss: 0.0004992352670058608\n",
            "Epoch 26 - Training loss: 0.00048395334724336863\n",
            "Epoch 27 - Training loss: 0.0004612623006105423\n",
            "Epoch 28 - Training loss: 0.00044003422409296034\n",
            "Epoch 29 - Training loss: 0.0004156926169991493\n",
            "Epoch 30 - Training loss: 0.0004144859978929162\n",
            "Epoch 31 - Training loss: 0.0004149078380316496\n",
            "Epoch 32 - Training loss: 0.00040503786858171227\n",
            "Epoch 33 - Training loss: 0.0003974840484559536\n",
            "Epoch 34 - Training loss: 0.0004090239444747567\n",
            "Epoch 35 - Training loss: 0.00038231980921700595\n",
            "Epoch 36 - Training loss: 0.0003772577295079827\n",
            "Epoch 37 - Training loss: 0.00037389945704489945\n",
            "Epoch 38 - Training loss: 0.00036165600959211586\n",
            "Epoch 39 - Training loss: 0.00033502606954425576\n",
            "Epoch 40 - Training loss: 0.0003242252636700869\n",
            "Epoch 41 - Training loss: 0.00029501712191849945\n",
            "Epoch 42 - Training loss: 0.0002965264773927629\n",
            "Epoch 43 - Training loss: 0.00029899757113307715\n",
            "Epoch 44 - Training loss: 0.00029710574606433513\n",
            "Epoch 45 - Training loss: 0.0002946085320785642\n",
            "Epoch 46 - Training loss: 0.0002854108529165387\n",
            "Epoch 47 - Training loss: 0.000289347492903471\n",
            "Epoch 48 - Training loss: 0.00030269570611417295\n",
            "Epoch 49 - Training loss: 0.0002698445176705718\n",
            "Epoch 50 - Training loss: 0.00028561735060065983\n",
            "Epoch 51 - Training loss: 0.00027851379960775374\n",
            "Epoch 52 - Training loss: 0.00027776540368795396\n",
            "Epoch 53 - Training loss: 0.0003000916428864002\n",
            "Epoch 54 - Training loss: 0.0002512549176812172\n",
            "Epoch 55 - Training loss: 0.0002624204946681857\n",
            "Epoch 56 - Training loss: 0.00025533482525497673\n",
            "Epoch 57 - Training loss: 0.00024023610474541783\n",
            "Epoch 58 - Training loss: 0.000250208193063736\n",
            "Epoch 59 - Training loss: 0.00022494441643357277\n",
            "Epoch 60 - Training loss: 0.0002316592212766409\n",
            "Epoch 61 - Training loss: 0.00023706141151487827\n",
            "Epoch 62 - Training loss: 0.00022205942701548337\n",
            "Epoch 63 - Training loss: 0.0002238198820501566\n",
            "Epoch 64 - Training loss: 0.00021982253156602383\n",
            "Epoch 65 - Training loss: 0.00021574997045099736\n",
            "Epoch 66 - Training loss: 0.00021560586784034967\n",
            "Epoch 67 - Training loss: 0.00021065133325755595\n",
            "Epoch 68 - Training loss: 0.0002176125113852322\n",
            "Epoch 69 - Training loss: 0.00022130757737904788\n",
            "\n",
            "Training Time (in minutes) = 108.69057350158691\n",
            "\n",
            "Model Accuracy = 88.62\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}