{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final solution",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbakhit/Modified-MNIST/blob/master/Final_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nJ47ABzStdv",
        "colab_type": "code",
        "outputId": "9668c2b5-b77d-4cc6-8d0b-8334dcd14be5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUsWBmaBg52B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Net, self).__init__()\n",
        "        # Defining a 2D convolution layer\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "        self.drop_out = nn.Dropout()\n",
        "\n",
        "        # Defining another 2D convolution layer\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(1*1*256, 4096)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(4096, 256)\n",
        "        )\n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-TGbFWTQ49",
        "colab_type": "code",
        "outputId": "d6e57248-834d-4318-8039-7f1e6cd49b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# FOR TESTING ON VALIDATION DATASET\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import inspect\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score, accuracy_score\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# number of times we iterate over the training set\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 256\n",
        "TRAIN_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_x'\n",
        "TEST_PATH = '/content/drive/My Drive/McGill/comp551/data/test_max_x'\n",
        "TARGETS_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_y.csv'\n",
        "THRESH = 240\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def calculate_metric(metric_fn, true_y, pred_y):\n",
        "    # multi class problems need to have averaging method\n",
        "    if \"average\" in inspect.getfullargspec(metric_fn).args:\n",
        "        return metric_fn(true_y, pred_y, average=\"macro\")\n",
        "    else:\n",
        "        return metric_fn(true_y, pred_y)\n",
        "    \n",
        "def print_scores(r, a, batch_size):\n",
        "    # just an utility printing function\n",
        "    for name, scores in zip((\"recall\", \"accuracy\"), (r, a)):\n",
        "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('torch.cuda.device_count(): {0}'.format(torch.cuda.device_count()))\n",
        "    print('torch.cuda.get_device_name: {0}'.format(torch.cuda.get_device_name(0)))\n",
        "    # load images as a numpy array\n",
        "    train_dataset = np.array(np.load(TRAIN_PATH, allow_pickle=True))\n",
        "    train_dataset = np.array([cv2.threshold(i, THRESH, 255, cv2.THRESH_BINARY)[1] for i in train_dataset])\n",
        "    train_dataset = train_dataset / 255.0\n",
        "\n",
        "    targets = pd.read_csv(TARGETS_PATH, delimiter=',', skipinitialspace=True)\n",
        "    targets = targets.to_numpy()\n",
        "    # remove id column\n",
        "    targets = targets[:, 1]\n",
        "    targets = targets.astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(train_dataset, targets, test_size=0.2, random_state=42)\n",
        "    # Clean memory\n",
        "    len_train_dataset = len(X_train)\n",
        "    train_dataset = None\n",
        "    targets = None\n",
        "\n",
        "    X_train = torch.from_numpy(X_train)\n",
        "    y_train = torch.from_numpy(y_train)\n",
        "    X_test = torch.from_numpy(X_test)\n",
        "    y_test = torch.from_numpy(y_test)\n",
        "    y_train = y_train.long()\n",
        "    y_test = y_test.long()\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test,y_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "\n",
        "    # defining the model\n",
        "    model = Net().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    time0 = time()\n",
        "    batches = len(train_loader)\n",
        "    val_batches = len(test_loader)\n",
        "    train_losses_arr = []\n",
        "    val_losses_arr = []\n",
        "    train_acc_arr = []\n",
        "    val_acc_arr = []\n",
        "\n",
        "    for e in range(EPOCHS):\n",
        "      running_loss = 0.0\n",
        "\n",
        "      # set model to training\n",
        "      model.train()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # clearing the Gradients of the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prediction for training\n",
        "        output_train = model(inputs)\n",
        "        \n",
        "        # computing the training loss\n",
        "        loss_train = criterion(output_train, labels)\n",
        "\n",
        "        # computing the updated weights of all the model parameters\n",
        "        loss_train.backward()\n",
        "\n",
        "        # And optimizes its weights here\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_train.item()\n",
        "\n",
        "        _, predicted = torch.max(output_train.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "      # releasing unnecessary memory in GPU\n",
        "      if torch.cuda.is_available():\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      # ----------------- VALIDATION  ----------------- \n",
        "      val_loss = 0\n",
        "      recall, accuracy = [], []\n",
        "      \n",
        "      # set model to evaluating (testing)\n",
        "      model.eval()\n",
        "\n",
        "      # prediction for validation set\n",
        "      # correct = 0\n",
        "      # total = 0\n",
        "      with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            output = model(images)\n",
        "            val_loss += criterion(output, labels)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            # total += labels.size(0)\n",
        "            # correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            for acc, metric in zip((recall, accuracy), \n",
        "                                   (recall_score, accuracy_score)):\n",
        "                acc.append(\n",
        "                    calculate_metric(metric, labels.cpu(), predicted.cpu())\n",
        "                )\n",
        "      # print(\"Epoch {} - Training loss: {} - Validation accuracy: {}\".format(e, running_loss / len_train_dataset, 100 * correct / total))\n",
        "      train_acc = correct/total\n",
        "      val_acc = sum(accuracy)/len(accuracy)\n",
        "      print(f\"Epoch {e+1}/{EPOCHS}, training accuarcy: {train_acc}, validation accuarcy: {val_acc}\")\n",
        "      print(f\"Epoch {e+1}/{EPOCHS}, training loss: {running_loss/batches}, validation loss: {val_loss/val_batches}\")\n",
        "      print_scores(recall, accuracy, val_batches)\n",
        "      train_losses_arr.append(running_loss/batches) # for plotting learning curve\n",
        "      val_losses_arr.append(val_loss/val_batches)\n",
        "      train_acc_arr.append(train_acc)\n",
        "      val_acc_arr.append(val_acc)\n",
        "\n",
        "    print(\"\\nTraining Time (in minutes) =\", (time() - time0) / 60)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_acc_arr)\n",
        "    plt.plot(val_acc_arr)\n",
        "    plt.title('Model accuracy')\n",
        "    plt.xlabel('Epoch Number')\n",
        "    plt.ylabel('Accuracy [%]')\n",
        "    x=range(0, EPOCHS+1, 5)\n",
        "    y=np.arange(0, 1.001, 0.05)\n",
        "    plt.yticks(y)\n",
        "    plt.xticks(x)\n",
        "    plt.legend(['training set', 'validation set'], loc='lower right')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_losses_arr)\n",
        "    plt.plot(val_losses_arr)\n",
        "    plt.title('Cross-Entropy Loss')\n",
        "    plt.xlabel('Epoch Number')\n",
        "\n",
        "    x=range(0, EPOCHS+1, 5)\n",
        "    plt.xticks(x)\n",
        "    plt.legend(['training set', 'validation set'], loc='upper right')\n",
        "\n",
        "    plt.show()\n",
        "    # # accuracy on validation set\n",
        "    # print(\"\\nModel Accuracy =\", (100 * correct / total))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.cuda.device_count(): 1\n",
            "torch.cuda.get_device_name: Tesla P100-PCIE-16GB\n",
            "Epoch 1/100, training accuarcy: 0.24865, validation accuarcy: 0.2673828125\n",
            "Epoch 1/100, training loss: 2.122849473528042, validation loss: 1.8944453001022339\n",
            "\t        recall: 0.1131\n",
            "\t      accuracy: 0.2674\n",
            "Epoch 2/100, training accuarcy: 0.321325, validation accuarcy: 0.23642578125\n",
            "Epoch 2/100, training loss: 1.8145782841239007, validation loss: 1.9849318265914917\n",
            "\t        recall: 0.1420\n",
            "\t      accuracy: 0.2364\n",
            "Epoch 3/100, training accuarcy: 0.438975, validation accuarcy: 0.42470703125\n",
            "Epoch 3/100, training loss: 1.4950450810657185, validation loss: 1.5570049285888672\n",
            "\t        recall: 0.2664\n",
            "\t      accuracy: 0.4247\n",
            "Epoch 4/100, training accuarcy: 0.56155, validation accuarcy: 0.6318359375\n",
            "Epoch 4/100, training loss: 1.2173518147438196, validation loss: 1.0519920587539673\n",
            "\t        recall: 0.3747\n",
            "\t      accuracy: 0.6318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/100, training accuarcy: 0.68915, validation accuarcy: 0.49541015625\n",
            "Epoch 5/100, training loss: 0.8987061540791943, validation loss: 2.0878102779388428\n",
            "\t        recall: 0.3004\n",
            "\t      accuracy: 0.4954\n",
            "Epoch 6/100, training accuarcy: 0.76365, validation accuarcy: 0.761328125\n",
            "Epoch 6/100, training loss: 0.6975955283565886, validation loss: 0.6825003027915955\n",
            "\t        recall: 0.5627\n",
            "\t      accuracy: 0.7613\n",
            "Epoch 7/100, training accuarcy: 0.8107, validation accuarcy: 0.809765625\n",
            "Epoch 7/100, training loss: 0.575632599129039, validation loss: 0.6292173266410828\n",
            "\t        recall: 0.5857\n",
            "\t      accuracy: 0.8098\n",
            "Epoch 8/100, training accuarcy: 0.84755, validation accuarcy: 0.83134765625\n",
            "Epoch 8/100, training loss: 0.48049382761025883, validation loss: 0.6193951368331909\n",
            "\t        recall: 0.6236\n",
            "\t      accuracy: 0.8313\n",
            "Epoch 9/100, training accuarcy: 0.875325, validation accuarcy: 0.8078125\n",
            "Epoch 9/100, training loss: 0.41348554981741936, validation loss: 0.6238025426864624\n",
            "\t        recall: 0.6355\n",
            "\t      accuracy: 0.8078\n",
            "Epoch 10/100, training accuarcy: 0.89125, validation accuarcy: 0.8609375\n",
            "Epoch 10/100, training loss: 0.358848845408221, validation loss: 0.45272204279899597\n",
            "\t        recall: 0.6789\n",
            "\t      accuracy: 0.8609\n",
            "Epoch 11/100, training accuarcy: 0.90445, validation accuarcy: 0.884375\n",
            "Epoch 11/100, training loss: 0.3140147532437258, validation loss: 0.44861075282096863\n",
            "\t        recall: 0.7196\n",
            "\t      accuracy: 0.8844\n",
            "Epoch 12/100, training accuarcy: 0.913225, validation accuarcy: 0.82431640625\n",
            "Epoch 12/100, training loss: 0.28958804582714276, validation loss: 0.6523504853248596\n",
            "\t        recall: 0.7130\n",
            "\t      accuracy: 0.8243\n",
            "Epoch 13/100, training accuarcy: 0.921125, validation accuarcy: 0.88955078125\n",
            "Epoch 13/100, training loss: 0.26369913007803025, validation loss: 0.3999888002872467\n",
            "\t        recall: 0.7599\n",
            "\t      accuracy: 0.8896\n",
            "Epoch 14/100, training accuarcy: 0.931925, validation accuarcy: 0.90068359375\n",
            "Epoch 14/100, training loss: 0.2234380866881389, validation loss: 0.3968646824359894\n",
            "\t        recall: 0.7457\n",
            "\t      accuracy: 0.9007\n",
            "Epoch 15/100, training accuarcy: 0.93275, validation accuarcy: 0.89990234375\n",
            "Epoch 15/100, training loss: 0.22077838749073114, validation loss: 0.4265018105506897\n",
            "\t        recall: 0.7644\n",
            "\t      accuracy: 0.8999\n",
            "Epoch 16/100, training accuarcy: 0.9419, validation accuarcy: 0.884765625\n",
            "Epoch 16/100, training loss: 0.19409441345246733, validation loss: 0.4575904905796051\n",
            "\t        recall: 0.7425\n",
            "\t      accuracy: 0.8848\n",
            "Epoch 17/100, training accuarcy: 0.947225, validation accuarcy: 0.916796875\n",
            "Epoch 17/100, training loss: 0.18275319405232265, validation loss: 0.3664071559906006\n",
            "\t        recall: 0.7766\n",
            "\t      accuracy: 0.9168\n",
            "Epoch 18/100, training accuarcy: 0.947775, validation accuarcy: 0.92041015625\n",
            "Epoch 18/100, training loss: 0.17931680186728763, validation loss: 0.32249876856803894\n",
            "\t        recall: 0.8002\n",
            "\t      accuracy: 0.9204\n",
            "Epoch 19/100, training accuarcy: 0.95605, validation accuarcy: 0.9208984375\n",
            "Epoch 19/100, training loss: 0.1515419508336456, validation loss: 0.3541758954524994\n",
            "\t        recall: 0.8294\n",
            "\t      accuracy: 0.9209\n",
            "Epoch 20/100, training accuarcy: 0.952575, validation accuarcy: 0.92626953125\n",
            "Epoch 20/100, training loss: 0.16829937981192472, validation loss: 0.3549649119377136\n",
            "\t        recall: 0.8192\n",
            "\t      accuracy: 0.9263\n",
            "Epoch 21/100, training accuarcy: 0.958975, validation accuarcy: 0.91806640625\n",
            "Epoch 21/100, training loss: 0.14050936743996706, validation loss: 0.37097087502479553\n",
            "\t        recall: 0.8317\n",
            "\t      accuracy: 0.9181\n",
            "Epoch 22/100, training accuarcy: 0.964525, validation accuarcy: 0.91474609375\n",
            "Epoch 22/100, training loss: 0.12408720542955551, validation loss: 0.3988713324069977\n",
            "\t        recall: 0.8178\n",
            "\t      accuracy: 0.9147\n",
            "Epoch 23/100, training accuarcy: 0.9639, validation accuarcy: 0.91904296875\n",
            "Epoch 23/100, training loss: 0.12634758705830879, validation loss: 0.3774190843105316\n",
            "\t        recall: 0.8121\n",
            "\t      accuracy: 0.9190\n",
            "Epoch 24/100, training accuarcy: 0.967675, validation accuarcy: 0.92294921875\n",
            "Epoch 24/100, training loss: 0.11466935348169059, validation loss: 0.37012794613838196\n",
            "\t        recall: 0.8427\n",
            "\t      accuracy: 0.9229\n",
            "Epoch 25/100, training accuarcy: 0.9717, validation accuarcy: 0.91826171875\n",
            "Epoch 25/100, training loss: 0.09867496959343078, validation loss: 0.4030299186706543\n",
            "\t        recall: 0.8072\n",
            "\t      accuracy: 0.9183\n",
            "Epoch 26/100, training accuarcy: 0.971675, validation accuarcy: 0.90078125\n",
            "Epoch 26/100, training loss: 0.10351294054869253, validation loss: 0.5139126181602478\n",
            "\t        recall: 0.8152\n",
            "\t      accuracy: 0.9008\n",
            "Epoch 27/100, training accuarcy: 0.9739, validation accuarcy: 0.92470703125\n",
            "Epoch 27/100, training loss: 0.09271728095545131, validation loss: 0.4684605598449707\n",
            "\t        recall: 0.8295\n",
            "\t      accuracy: 0.9247\n",
            "Epoch 28/100, training accuarcy: 0.972325, validation accuarcy: 0.9357421875\n",
            "Epoch 28/100, training loss: 0.10120394970675942, validation loss: 0.347452849149704\n",
            "\t        recall: 0.8555\n",
            "\t      accuracy: 0.9357\n",
            "Epoch 29/100, training accuarcy: 0.9748, validation accuarcy: 0.93271484375\n",
            "Epoch 29/100, training loss: 0.0903975639705825, validation loss: 0.389588862657547\n",
            "\t        recall: 0.8444\n",
            "\t      accuracy: 0.9327\n",
            "Epoch 30/100, training accuarcy: 0.978575, validation accuarcy: 0.93603515625\n",
            "Epoch 30/100, training loss: 0.07642648690587776, validation loss: 0.41360974311828613\n",
            "\t        recall: 0.8491\n",
            "\t      accuracy: 0.9360\n",
            "Epoch 31/100, training accuarcy: 0.9765, validation accuarcy: 0.93544921875\n",
            "Epoch 31/100, training loss: 0.08619815107601084, validation loss: 0.3669111430644989\n",
            "\t        recall: 0.8567\n",
            "\t      accuracy: 0.9354\n",
            "Epoch 32/100, training accuarcy: 0.979325, validation accuarcy: 0.9369140625\n",
            "Epoch 32/100, training loss: 0.07428174378101234, validation loss: 0.3866509199142456\n",
            "\t        recall: 0.8467\n",
            "\t      accuracy: 0.9369\n",
            "Epoch 33/100, training accuarcy: 0.9673, validation accuarcy: 0.9236328125\n",
            "Epoch 33/100, training loss: 0.1314420795578296, validation loss: 0.3622218072414398\n",
            "\t        recall: 0.8457\n",
            "\t      accuracy: 0.9236\n",
            "Epoch 34/100, training accuarcy: 0.9774, validation accuarcy: 0.93984375\n",
            "Epoch 34/100, training loss: 0.08640669171170444, validation loss: 0.38402852416038513\n",
            "\t        recall: 0.8672\n",
            "\t      accuracy: 0.9398\n",
            "Epoch 35/100, training accuarcy: 0.980325, validation accuarcy: 0.9380859375\n",
            "Epoch 35/100, training loss: 0.07196851221808961, validation loss: 0.37279874086380005\n",
            "\t        recall: 0.8774\n",
            "\t      accuracy: 0.9381\n",
            "Epoch 36/100, training accuarcy: 0.982875, validation accuarcy: 0.93818359375\n",
            "Epoch 36/100, training loss: 0.06193217950736641, validation loss: 0.3648212254047394\n",
            "\t        recall: 0.8612\n",
            "\t      accuracy: 0.9382\n",
            "Epoch 37/100, training accuarcy: 0.98485, validation accuarcy: 0.939453125\n",
            "Epoch 37/100, training loss: 0.05913517334658629, validation loss: 0.3768536150455475\n",
            "\t        recall: 0.8561\n",
            "\t      accuracy: 0.9395\n",
            "Epoch 38/100, training accuarcy: 0.985475, validation accuarcy: 0.94345703125\n",
            "Epoch 38/100, training loss: 0.0531010044928474, validation loss: 0.4458830952644348\n",
            "\t        recall: 0.8720\n",
            "\t      accuracy: 0.9435\n",
            "Epoch 39/100, training accuarcy: 0.984625, validation accuarcy: 0.92763671875\n",
            "Epoch 39/100, training loss: 0.05797014529609187, validation loss: 0.599973738193512\n",
            "\t        recall: 0.8487\n",
            "\t      accuracy: 0.9276\n",
            "Epoch 40/100, training accuarcy: 0.98435, validation accuarcy: 0.93994140625\n",
            "Epoch 40/100, training loss: 0.058613524285803555, validation loss: 0.4228088855743408\n",
            "\t        recall: 0.8636\n",
            "\t      accuracy: 0.9399\n",
            "Epoch 41/100, training accuarcy: 0.986225, validation accuarcy: 0.93662109375\n",
            "Epoch 41/100, training loss: 0.04970794685398507, validation loss: 0.4473409652709961\n",
            "\t        recall: 0.8555\n",
            "\t      accuracy: 0.9366\n",
            "Epoch 42/100, training accuarcy: 0.986325, validation accuarcy: 0.92900390625\n",
            "Epoch 42/100, training loss: 0.0509912366415285, validation loss: 0.5842956900596619\n",
            "\t        recall: 0.8371\n",
            "\t      accuracy: 0.9290\n",
            "Epoch 43/100, training accuarcy: 0.9824, validation accuarcy: 0.9376953125\n",
            "Epoch 43/100, training loss: 0.06912215743332532, validation loss: 0.40144625306129456\n",
            "\t        recall: 0.8783\n",
            "\t      accuracy: 0.9377\n",
            "Epoch 44/100, training accuarcy: 0.9853, validation accuarcy: 0.9396484375\n",
            "Epoch 44/100, training loss: 0.0553569022008473, validation loss: 0.38279685378074646\n",
            "\t        recall: 0.8633\n",
            "\t      accuracy: 0.9396\n",
            "Epoch 45/100, training accuarcy: 0.98815, validation accuarcy: 0.9392578125\n",
            "Epoch 45/100, training loss: 0.043322088985828455, validation loss: 0.441158264875412\n",
            "\t        recall: 0.8635\n",
            "\t      accuracy: 0.9393\n",
            "Epoch 46/100, training accuarcy: 0.99065, validation accuarcy: 0.93662109375\n",
            "Epoch 46/100, training loss: 0.03376710882327359, validation loss: 0.4340622127056122\n",
            "\t        recall: 0.8745\n",
            "\t      accuracy: 0.9366\n",
            "Epoch 47/100, training accuarcy: 0.988425, validation accuarcy: 0.93759765625\n",
            "Epoch 47/100, training loss: 0.04349915949615893, validation loss: 0.4638521373271942\n",
            "\t        recall: 0.8637\n",
            "\t      accuracy: 0.9376\n",
            "Epoch 48/100, training accuarcy: 0.987575, validation accuarcy: 0.928515625\n",
            "Epoch 48/100, training loss: 0.04686096635688642, validation loss: 0.4380772113800049\n",
            "\t        recall: 0.8688\n",
            "\t      accuracy: 0.9285\n",
            "Epoch 49/100, training accuarcy: 0.9866, validation accuarcy: 0.93876953125\n",
            "Epoch 49/100, training loss: 0.05323495786447244, validation loss: 0.4415481984615326\n",
            "\t        recall: 0.8777\n",
            "\t      accuracy: 0.9388\n",
            "Epoch 50/100, training accuarcy: 0.98665, validation accuarcy: 0.93896484375\n",
            "Epoch 50/100, training loss: 0.053628695499934965, validation loss: 0.4441129267215729\n",
            "\t        recall: 0.8721\n",
            "\t      accuracy: 0.9390\n",
            "Epoch 51/100, training accuarcy: 0.988925, validation accuarcy: 0.9412109375\n",
            "Epoch 51/100, training loss: 0.043605990173995114, validation loss: 0.4360496997833252\n",
            "\t        recall: 0.8696\n",
            "\t      accuracy: 0.9412\n",
            "Epoch 52/100, training accuarcy: 0.988175, validation accuarcy: 0.94365234375\n",
            "Epoch 52/100, training loss: 0.04633356510382739, validation loss: 0.425103098154068\n",
            "\t        recall: 0.8825\n",
            "\t      accuracy: 0.9437\n",
            "Epoch 53/100, training accuarcy: 0.9909, validation accuarcy: 0.9421875\n",
            "Epoch 53/100, training loss: 0.037273294227138445, validation loss: 0.4383082985877991\n",
            "\t        recall: 0.8782\n",
            "\t      accuracy: 0.9422\n",
            "Epoch 54/100, training accuarcy: 0.988275, validation accuarcy: 0.94033203125\n",
            "Epoch 54/100, training loss: 0.04592269720735064, validation loss: 0.4388945996761322\n",
            "\t        recall: 0.8662\n",
            "\t      accuracy: 0.9403\n",
            "Epoch 55/100, training accuarcy: 0.991275, validation accuarcy: 0.93486328125\n",
            "Epoch 55/100, training loss: 0.03465835784508544, validation loss: 0.5103561282157898\n",
            "\t        recall: 0.8375\n",
            "\t      accuracy: 0.9349\n",
            "Epoch 56/100, training accuarcy: 0.9897, validation accuarcy: 0.91875\n",
            "Epoch 56/100, training loss: 0.04083681934673315, validation loss: 0.6586841344833374\n",
            "\t        recall: 0.8125\n",
            "\t      accuracy: 0.9187\n",
            "Epoch 57/100, training accuarcy: 0.9856, validation accuarcy: 0.94404296875\n",
            "Epoch 57/100, training loss: 0.055472183266690206, validation loss: 0.44308313727378845\n",
            "\t        recall: 0.8723\n",
            "\t      accuracy: 0.9440\n",
            "Epoch 58/100, training accuarcy: 0.991075, validation accuarcy: 0.9408203125\n",
            "Epoch 58/100, training loss: 0.03642035978045433, validation loss: 0.499196857213974\n",
            "\t        recall: 0.8613\n",
            "\t      accuracy: 0.9408\n",
            "Epoch 59/100, training accuarcy: 0.99235, validation accuarcy: 0.94013671875\n",
            "Epoch 59/100, training loss: 0.03144393430631252, validation loss: 0.4923117160797119\n",
            "\t        recall: 0.8698\n",
            "\t      accuracy: 0.9401\n",
            "Epoch 60/100, training accuarcy: 0.990575, validation accuarcy: 0.93974609375\n",
            "Epoch 60/100, training loss: 0.03621217204483262, validation loss: 0.46118900179862976\n",
            "\t        recall: 0.8627\n",
            "\t      accuracy: 0.9397\n",
            "Epoch 61/100, training accuarcy: 0.992, validation accuarcy: 0.933203125\n",
            "Epoch 61/100, training loss: 0.032857363924953586, validation loss: 0.675243079662323\n",
            "\t        recall: 0.8562\n",
            "\t      accuracy: 0.9332\n",
            "Epoch 62/100, training accuarcy: 0.989575, validation accuarcy: 0.9357421875\n",
            "Epoch 62/100, training loss: 0.03955480342458008, validation loss: 0.4784823954105377\n",
            "\t        recall: 0.8569\n",
            "\t      accuracy: 0.9357\n",
            "Epoch 63/100, training accuarcy: 0.9913, validation accuarcy: 0.9451171875\n",
            "Epoch 63/100, training loss: 0.035855076999468785, validation loss: 0.460681289434433\n",
            "\t        recall: 0.8796\n",
            "\t      accuracy: 0.9451\n",
            "Epoch 64/100, training accuarcy: 0.991625, validation accuarcy: 0.94189453125\n",
            "Epoch 64/100, training loss: 0.03397298125538287, validation loss: 0.45646506547927856\n",
            "\t        recall: 0.8701\n",
            "\t      accuracy: 0.9419\n",
            "Epoch 65/100, training accuarcy: 0.990325, validation accuarcy: 0.93974609375\n",
            "Epoch 65/100, training loss: 0.04032126411344785, validation loss: 0.438527911901474\n",
            "\t        recall: 0.8653\n",
            "\t      accuracy: 0.9397\n",
            "Epoch 66/100, training accuarcy: 0.991075, validation accuarcy: 0.94228515625\n",
            "Epoch 66/100, training loss: 0.0352619454134137, validation loss: 0.43332067131996155\n",
            "\t        recall: 0.8569\n",
            "\t      accuracy: 0.9423\n",
            "Epoch 67/100, training accuarcy: 0.9932, validation accuarcy: 0.9384765625\n",
            "Epoch 67/100, training loss: 0.026887104644849422, validation loss: 0.5164253115653992\n",
            "\t        recall: 0.8812\n",
            "\t      accuracy: 0.9385\n",
            "Epoch 68/100, training accuarcy: 0.9926, validation accuarcy: 0.94560546875\n",
            "Epoch 68/100, training loss: 0.030323978721098915, validation loss: 0.45686593651771545\n",
            "\t        recall: 0.8741\n",
            "\t      accuracy: 0.9456\n",
            "Epoch 69/100, training accuarcy: 0.994175, validation accuarcy: 0.93984375\n",
            "Epoch 69/100, training loss: 0.02199041542069168, validation loss: 0.4929758608341217\n",
            "\t        recall: 0.8649\n",
            "\t      accuracy: 0.9398\n",
            "Epoch 70/100, training accuarcy: 0.991275, validation accuarcy: 0.9380859375\n",
            "Epoch 70/100, training loss: 0.03399380604339063, validation loss: 0.4834226667881012\n",
            "\t        recall: 0.8794\n",
            "\t      accuracy: 0.9381\n",
            "Epoch 71/100, training accuarcy: 0.99145, validation accuarcy: 0.93759765625\n",
            "Epoch 71/100, training loss: 0.03154138402099822, validation loss: 0.49460169672966003\n",
            "\t        recall: 0.8616\n",
            "\t      accuracy: 0.9376\n",
            "Epoch 72/100, training accuarcy: 0.9909, validation accuarcy: 0.93935546875\n",
            "Epoch 72/100, training loss: 0.03581756461340531, validation loss: 0.4834170341491699\n",
            "\t        recall: 0.8788\n",
            "\t      accuracy: 0.9394\n",
            "Epoch 73/100, training accuarcy: 0.990925, validation accuarcy: 0.94501953125\n",
            "Epoch 73/100, training loss: 0.038317626490477166, validation loss: 0.5310366749763489\n",
            "\t        recall: 0.8749\n",
            "\t      accuracy: 0.9450\n",
            "Epoch 74/100, training accuarcy: 0.993675, validation accuarcy: 0.9361328125\n",
            "Epoch 74/100, training loss: 0.027444646331914672, validation loss: 0.5586760640144348\n",
            "\t        recall: 0.8740\n",
            "\t      accuracy: 0.9361\n",
            "Epoch 75/100, training accuarcy: 0.99225, validation accuarcy: 0.94013671875\n",
            "Epoch 75/100, training loss: 0.030440441777656792, validation loss: 0.5236029028892517\n",
            "\t        recall: 0.8647\n",
            "\t      accuracy: 0.9401\n",
            "Epoch 76/100, training accuarcy: 0.991975, validation accuarcy: 0.94033203125\n",
            "Epoch 76/100, training loss: 0.030445705001853455, validation loss: 0.5211600661277771\n",
            "\t        recall: 0.8671\n",
            "\t      accuracy: 0.9403\n",
            "Epoch 77/100, training accuarcy: 0.99305, validation accuarcy: 0.93974609375\n",
            "Epoch 77/100, training loss: 0.02793473136984998, validation loss: 0.516395628452301\n",
            "\t        recall: 0.8591\n",
            "\t      accuracy: 0.9397\n",
            "Epoch 78/100, training accuarcy: 0.99315, validation accuarcy: 0.94052734375\n",
            "Epoch 78/100, training loss: 0.026946473255706063, validation loss: 0.4801606833934784\n",
            "\t        recall: 0.8705\n",
            "\t      accuracy: 0.9405\n",
            "Epoch 79/100, training accuarcy: 0.992975, validation accuarcy: 0.93828125\n",
            "Epoch 79/100, training loss: 0.02784410377691506, validation loss: 0.5303972363471985\n",
            "\t        recall: 0.8673\n",
            "\t      accuracy: 0.9383\n",
            "Epoch 80/100, training accuarcy: 0.994075, validation accuarcy: 0.944140625\n",
            "Epoch 80/100, training loss: 0.0251201992842612, validation loss: 0.527084231376648\n",
            "\t        recall: 0.8809\n",
            "\t      accuracy: 0.9441\n",
            "Epoch 81/100, training accuarcy: 0.990325, validation accuarcy: 0.93095703125\n",
            "Epoch 81/100, training loss: 0.04854537174698843, validation loss: 0.48521170020103455\n",
            "\t        recall: 0.8561\n",
            "\t      accuracy: 0.9310\n",
            "Epoch 82/100, training accuarcy: 0.989325, validation accuarcy: 0.93310546875\n",
            "Epoch 82/100, training loss: 0.0442895670095162, validation loss: 0.5370301008224487\n",
            "\t        recall: 0.8554\n",
            "\t      accuracy: 0.9331\n",
            "Epoch 83/100, training accuarcy: 0.994875, validation accuarcy: 0.94267578125\n",
            "Epoch 83/100, training loss: 0.021835697743638305, validation loss: 0.5193424820899963\n",
            "\t        recall: 0.8780\n",
            "\t      accuracy: 0.9427\n",
            "Epoch 84/100, training accuarcy: 0.993975, validation accuarcy: 0.936328125\n",
            "Epoch 84/100, training loss: 0.023743592345031202, validation loss: 0.548865020275116\n",
            "\t        recall: 0.8596\n",
            "\t      accuracy: 0.9363\n",
            "Epoch 85/100, training accuarcy: 0.994725, validation accuarcy: 0.939453125\n",
            "Epoch 85/100, training loss: 0.021083871703476285, validation loss: 0.559994637966156\n",
            "\t        recall: 0.8752\n",
            "\t      accuracy: 0.9395\n",
            "Epoch 86/100, training accuarcy: 0.99485, validation accuarcy: 0.937890625\n",
            "Epoch 86/100, training loss: 0.0215258249229971, validation loss: 0.4826178252696991\n",
            "\t        recall: 0.8629\n",
            "\t      accuracy: 0.9379\n",
            "Epoch 87/100, training accuarcy: 0.9955, validation accuarcy: 0.9373046875\n",
            "Epoch 87/100, training loss: 0.018723761342513332, validation loss: 0.5755771994590759\n",
            "\t        recall: 0.8691\n",
            "\t      accuracy: 0.9373\n",
            "Epoch 88/100, training accuarcy: 0.993675, validation accuarcy: 0.943359375\n",
            "Epoch 88/100, training loss: 0.024597827635800384, validation loss: 0.48988109827041626\n",
            "\t        recall: 0.8676\n",
            "\t      accuracy: 0.9434\n",
            "Epoch 89/100, training accuarcy: 0.996275, validation accuarcy: 0.94775390625\n",
            "Epoch 89/100, training loss: 0.014856829557117004, validation loss: 0.5602285265922546\n",
            "\t        recall: 0.8760\n",
            "\t      accuracy: 0.9478\n",
            "Epoch 90/100, training accuarcy: 0.9944, validation accuarcy: 0.9359375\n",
            "Epoch 90/100, training loss: 0.022706720248386738, validation loss: 0.576282799243927\n",
            "\t        recall: 0.8773\n",
            "\t      accuracy: 0.9359\n",
            "Epoch 91/100, training accuarcy: 0.99175, validation accuarcy: 0.9390625\n",
            "Epoch 91/100, training loss: 0.035594737258069456, validation loss: 0.4821847379207611\n",
            "\t        recall: 0.8656\n",
            "\t      accuracy: 0.9391\n",
            "Epoch 92/100, training accuarcy: 0.994975, validation accuarcy: 0.94453125\n",
            "Epoch 92/100, training loss: 0.019086526000907846, validation loss: 0.4855031967163086\n",
            "\t        recall: 0.8789\n",
            "\t      accuracy: 0.9445\n",
            "Epoch 93/100, training accuarcy: 0.996, validation accuarcy: 0.9396484375\n",
            "Epoch 93/100, training loss: 0.015348772423424919, validation loss: 0.5230898261070251\n",
            "\t        recall: 0.8683\n",
            "\t      accuracy: 0.9396\n",
            "Epoch 94/100, training accuarcy: 0.995175, validation accuarcy: 0.94677734375\n",
            "Epoch 94/100, training loss: 0.019745904388748535, validation loss: 0.5068504810333252\n",
            "\t        recall: 0.8753\n",
            "\t      accuracy: 0.9468\n",
            "Epoch 95/100, training accuarcy: 0.9939, validation accuarcy: 0.94580078125\n",
            "Epoch 95/100, training loss: 0.024791369232449942, validation loss: 0.4935024380683899\n",
            "\t        recall: 0.8679\n",
            "\t      accuracy: 0.9458\n",
            "Epoch 96/100, training accuarcy: 0.995525, validation accuarcy: 0.94326171875\n",
            "Epoch 96/100, training loss: 0.017136461540439706, validation loss: 0.5277210474014282\n",
            "\t        recall: 0.8781\n",
            "\t      accuracy: 0.9433\n",
            "Epoch 97/100, training accuarcy: 0.995575, validation accuarcy: 0.934765625\n",
            "Epoch 97/100, training loss: 0.01959719088308181, validation loss: 0.5474021434783936\n",
            "\t        recall: 0.8614\n",
            "\t      accuracy: 0.9348\n",
            "Epoch 98/100, training accuarcy: 0.992775, validation accuarcy: 0.92900390625\n",
            "Epoch 98/100, training loss: 0.029805673909177825, validation loss: 0.55674809217453\n",
            "\t        recall: 0.8653\n",
            "\t      accuracy: 0.9290\n",
            "Epoch 99/100, training accuarcy: 0.993425, validation accuarcy: 0.9458984375\n",
            "Epoch 99/100, training loss: 0.02763572450917048, validation loss: 0.5101161003112793\n",
            "\t        recall: 0.8837\n",
            "\t      accuracy: 0.9459\n",
            "Epoch 100/100, training accuarcy: 0.994825, validation accuarcy: 0.9462890625\n",
            "Epoch 100/100, training loss: 0.02281640910774849, validation loss: 0.5193807482719421\n",
            "\t        recall: 0.8796\n",
            "\t      accuracy: 0.9463\n",
            "\n",
            "Training Time (in minutes) = 85.38282987674077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydeXhU5fX4P2cmk5VAwqbsQQRFdoyi\nohZ3XOpaFa2tWFtrta5fbbU/q5bWrbXWWvd9aVERlaJ1oxXcqFRQRJB9EcKaEAJZJ7Oc3x/vO8kk\nTJJJMkNCeD/PM0/ufZd73zu5c88957znvKKqOBwOh8PRFJ62HoDD4XA49g6cwHA4HA5HXDiB4XA4\nHI64cALD4XA4HHHhBIbD4XA44sIJDIfD4XDEhRMY7RQRyRMRFZGUONpOFpFP98S4HA7HvosTGAlA\nRNaJSLWIdK9X/pV96Oe1zcgcjtiIyMUiMl9EykRks4i8KyJHt+F4Ii9IZfU+F8bZX0XkwGSPMx46\n8gucExiJYy1wUWRHREYAmW03nPZBPBqSY88iIjcCDwJ3A/sB/YFHgbMaaL8n/4c5qtop6vNqIg7q\n7sPE4ARG4ngJ+HHU/qXAi9ENRKSLiLwoIoUi8p2I3CYiHlvnFZH7RaRIRNYAp8fo+4x9G9woIn8Q\nEW88AxOR10Rki4jsFJGPRWRYVF2GiPzZjmeniHwqIhm27mgRmSsiJSKyQUQm2/I5IvLTqGPUeaOy\nb3tXi8hKYKUt+6s9xi4RWSAix0S194rIb0RktYiU2vp+IvKIiPy53rXMFJEb4rlux+6ISBdgCnC1\nqr6hquWqGlDVt1T1ZtvmThGZLiJ/F5FdwGQRSRORB0Vkk/08KCJptn13EXnb3ifFIvJJ1H39a3u/\nlorIchE5oYXjft7eD/+yx5onIoNs3ce22dcRrUREJohIgT3/FuA52/ZnIrLKjnOmiPSOOoeKyLUi\nssb+Dv8kIh4RSbXtR0S17SkiFSLSo5nX0duet9iO42dRdYeL0fp2ichWEXnAlqfb/8V2+x1/ISL7\nteR7bDWq6j6t/ADrgBOB5cBQwAsUAAMABfJsuxeBfwLZQB6wArjc1l0JLAP6AV2B2bZviq1/E3gC\nyAJ6Av8Dfm7rJgOfNjK+n9hzpmHeLBdG1T0CzAH62HEfZdsNAEoxWpMP6AaMtn3mAD+NOkad89tx\nz7LXkWHLLrHHSAH+D9gCpNu6m4FvgIMAAUbZtocDmwCPbdcdqAD2a+v/+d76ASYCwch91UCbO4EA\ncDbmpTIDI2Q+t/deD2Au8Hvb/h7gcXuf+IBj7P/xIGAD0Nu2ywMGNXDOvOj7PUb988B2e0+kAP8A\nXql3zx0YtT/BXud99n7OAI4HioCxtuxvwMf1jjHb3rf9Mb/Pn9q6R4H7otpeB7zVwFjr/B7q1X1s\nj5UOjAYKgeNt3X+BH9ntTsARdvvnwFsYi4UXOBTo3Cb3T1vfwB3hQ63AuM3+eCbaB2aKvQnz7D+6\nGjgkqt/PgTl2+0Pgyqi6kyM/IIzZwI99+Nr6i4DZTd2gMcaaY4/bxT4MKoFRMdrdCrzZwDHm0LTA\nOL6JceyInBcjaM9qoN1S4CS7/Uvgnbb+f+/NH+CHwJYm2twZ/SC1ZauB06L2TwHW2e0pmBehA+v1\nORDYZn8bvibOmWfvm5J6n6G2/nng6aj2pwHL6t1z9QVGNfalxJY9A/wxar8TRjDmRR1jYlT9VcB/\n7PY4YD0gdn8+cEED1xLz94h5GQwB2VFl9wDP2+2Pgd8B3ev1+wlGQI9s6/vHmaQSy0vAxZgb5sV6\ndd0xb1/fRZV9h3mzB+iNeRuLroswwPbdbFXSEoy20bOpAVlzz73W3LMLI9wi4+mOedNZHaNrvwbK\n4yX6WhCRm0RkqTV7lWAEVmSSQGPnegGjnWD/vtSKMTnMW3p3adqmv6Hefm92v3cj5pw/AauAD6w5\n5xYAVV0FXI8RQNtE5JWICUjqOrb7Rx23u6rmRH2WRtVtidquwDzwG6NQVasaugZVLcN8H32i2tT/\nDfa2befZc04QkYMxwnBmE+evT2+gWFVL650jcv7LgSHAMmt2OsOWvwS8D7xizYF/FBFfM8+dEJzA\nSCCq+h3G+X0a8Ea96iLM28yAqLL+wEa7vRnz4Iyui7ABo2FE/5g6q+owmuZijDPzRMxDOs+Wix1T\nFTAoRr8NDZQDlFPXob9/jDY1aZCtv+JXwAVArqrmADvtGJo619+Bs0RkFMbcN6OBdo74+C/mXjq7\niXb101hvYvd7dxOAqpaq6v+p6gHAmcCNEV+Fqk5V1aOpNc/eZ8ujHdvrW3tRLbkGEcnCmD43RrWp\n/xvcFLUfeXn5ETC9njCKh01AVxHJrneOjQCqulJVL8K8CN4HTBeRLDU+pt+p6iEYk/EZ1PWX7jGc\nwEg8l2PMMeXRhaoaAqYBd4lItogMAG7EPBCxddeKSF8RyQVuieq7GfgA+LOIdLaOuEEi8r04xpON\neUBsxzzk7446bhh4FnjAOuO8InKkdWb+AzhRRC4QkRQR6SYio23XhcC5IpIpZirj5XGMIYix16aI\nyO1A56j6p4Hfi8hgMYwUkW52jAXAF5i3rNdVtTKOa3Y0gKruBG4HHhGRs+3/0Ccip4rIHxvp+jJw\nm4j0EDN9/HbsvSsiZ4jIgSIimBeBEBAWkYNE5Hh7P1VhzJ/hJF3aVuCAJtq8DFwmIqPtmO4G5qnq\nuqg2N4tIroj0w/gpomdp/R04ByM06lsQ6iPWWV3zUdUNGNPSPbZsJOa3E/keLxGRHvZ3WWKPExaR\n40RkhJhJLrswL57J+h4bp61tYh3hg/VhxCiv8WHY/VzMzVGIeau+nVqHbgrwF8yDfS1wNXWd3l2A\nxzDO9J3AV8AkWzeZhp1snTD25VKM+vtjouy9GGfgg5i3nJ0YO2rEUX0MMA9zk24ALrXl3TECrBT4\nDGNyqO/DiLYnezGCaRdGk/pV9Hdm62+z112KERB9o/pfYo95XFv/rzvKB+PLmI/RFrcA/wKOsnV3\nAn+v1z4deMj+/zbb7cikhRvs/7Pc3p+/teUjMZMzSoFi4G2sAzzGePLs/7is3udGW/888Ieo9hOA\ngqj9K+24SjCabJ36eu1WR40n+j5T4FpgDeZ3+GfAW6//v+21SiPf7WR7rPqfFKCvPW+xHUe03/Lv\nGJ9PGbAEONuWX4Tx85VjBONDNDJpIZmfiAPH4Wi3iMixmB/TAHU3rCNJiIgCg9X4Xhpq8yywSVVv\n23Mjaz+4YBZHu8Y6967DzJBxwsLRZojJ2HAuMKZtR9J2OB+Go90iIkMxJoZeGLOZw9EmiMjvgcXA\nn1R1bVuPp61wJimHw+FwxIXTMBwOh8MRFx3Gh9G9e3fNy8tr62E4OjALFiwoUtVm5Q5KBO7ediST\n5tzXHUZg5OXlMX/+/LYehqMDIyLfNd0q8bh725FMmnNfO5OUw+FwOOLCCQyHw+FwxEXSBIaIPCsi\n20RkcQP1IiIP2Zzwi0RkbFTdpSKy0n4uTdYYHQ6HwxE/yfRhPA88TMM5V04FBtvPOEzai3Ei0hW4\nA8jHhNMvEJGZqrojiWN1OBwJIBAIUFBQQFVVc/PyOZJNeno6ffv2xedreaLbpAkMVf1YGl/L+izg\nRRu9+7mI5IhIL0wOmFmqWgwgIrMw60u8nKyxOhyOxFBQUEB2djZ5eXmYXISO9oCqsn37dgoKChg4\ncGCLj9OWs6T6UDf3fIEta6jcsY8TDIXxB8NkpaVQ5g9SUR2k3B9iZ2WAg/bLZltpFYsKdpKfl0uv\nLhkAFOyoYOXWMhAY0DWTAd2y2LKrivXbK/B5hVH9cvB5jWV2R3k1HhG6ZLbJUgMdgqqqKics2iEi\nQrdu3SgsLGzVcfbqabUicgVwBUD//v2baL3vUVEdpLDUT/+umY3+gDeVVBIKK31zM/hi3Q5SvMKI\nPl3weT1sKK5g9vJtbCqpYlTfLpRXh6ioDnL8wT3ZUFzJ+uJyQmEYsl8nwgqLCkr4dFURaSkeCkv9\nbCyp5Psje3Nwr84IkJvlQ0T4ZEUR89ZuZ8h+2ezXOZ2qQIjvtpdTFQjj8UAgpGworuCIA7px7QmD\nqQqEuOKl+WworiTd56EqUDe7c6rXQ3Wotix/QC6ZaSl8vKKw0Xb7dU5jcM9s1hdXsL64gttOH8pP\nj2kqS7ajMZywaJ8k4v/SlgJjI3UXK+lryzZizFLR5XNiHUBVnwSeBMjPz+/wOU7CYWVHRTUKdMtK\nRUTYvLOSpZt3oQpHHNCNrLQUvt20ixfmruPtRZsorw5xQPcshvXpwoCumfTNzUCB/67ezvItpWSm\neVm4oQRV8/DcussP9vg/HNefZz9bR5k/iEcgHPUN3/7PJQ2Oc3DPTohAdrqPEX1yeG7uOkLhuv8e\nn1cY2z+X/67eTnF5NSleYWD3LDJTvWgIRGBM/xz+9c1m3vzKrG/TMzuNG04cwq6qAD2y0+iUlkJm\nqpfMVC9frS+hc4aPIwd14/M123ltfgHrtpdz08lDGHdANwRYU1TOyq2l9M7J4KD9simpDDB9QQE7\nKqoZ0acLF4/rzzGD93hcXot55X/r8XiEC/L7Nd3Y4UgAbSkwZgK/FJFXME7vnaq6WUTeB+62iwiB\nWdv61rYaZFtTFQjxwbdbeevrTXy2qoiK6hAAqSkeslK97KgI1LTdr3MaA7pm8b91xWT4vHx/VC+G\n9urMRysK+XpDCe98s7nmwZ2b6WNs/1x2Vga45rgDSU/1smDdDm46eX+y0lJ4+pM1PPThKob17sxD\nF42hX24mizftJDstBRFhzvJtDOyexUH7Z6MKK7aW4vN6OKBHFn1zM+tcw47yasr8QYJhpaSimrDC\ngG6ZdO+U1uT1byypZPaybWwvq+bCw/qxf5f0mO0mDu9Vsz22fy5XTThwtzb5eV13KzttRK/dyvYW\n3vhyIyI4gRFFSUkJU6dO5aqrrmp239NOO42pU6eSk5PTYJvbb7+dY489lhNPPLE1w2w2M2bMYMiQ\nIRxyyCF79Lz1SZrAEJGXMZpCdxEpwMx88gGo6uPAO5ilTFdh1sq9zNYV28yQX9hDTYk4wDsiO8qr\nefrTNYQV9stO453FW7j2+MEcNjCXRz5cxUuff8eOigC9uqRz7tg+DO6ZTViVLTurKPMH6d81k/y8\nXMr8IR789wq2llZx66kHM+mw/jW2+MvGGydXIBRm6y4ze2W/zuk1tvtYTBy2P5+v3c6YfrlkpHoB\n8yCOcGDPussp9+taV0hEk5uVSm5Wqt3Latb30ycng0uOGNB0w32QrDQvRWXVbT2MdkVJSQmPPvpo\nTIERDAZJSWn4kffOO+80efwpU6a0anwtZcaMGZxxxhkdV2CoWZu2sXrFrCoXq+5ZzAptHYbNOysp\n2FHJppJKPlpeSI/OafgDYd74ssCafIRgWPF5hdtmfMNxB/fkuc/WcdIh+3HpkXkcNagbHk/jNsjv\nDWncnOLzenZ7+28Ij0c4alD3uK/PsefJTEuhvLhi94pdmyF7f2PX28e45ZZbWL16NaNHj+akk07i\n9NNP57e//S25ubksW7aMFStWcPbZZ7Nhwwaqqqq47rrruOKKK4DaFCxlZWWceuqpHH300cydO5c+\nffrwz3/+k4yMDCZPnswZZ5zBD37wA/Ly8rj00kt56623CAQCvPbaaxx88MEUFhZy8cUXs2nTJo48\n8khmzZrFggUL6N699vcUCoW4/PLLmT9/PiLCT37yE2644QZWr17N1VdfTWFhIZmZmTz11FMUFxcz\nc+ZMPvroI/7whz/w+uuvM2jQoDb5fvdqp/feQEV1kEdnr+aJj1cTCNWag8r8QQThpGH7ce3xg9m/\nczpbS6vYUFzB5S/M57nP1nHR4f2459yRbXwFjvZKVqqXcn+wbuHOAnhwBFz6FuQd3TYDs/zurSV8\nu2lXQo95SO/O3PH9YQ3W33vvvSxevJiFCxcCMGfOHL788ksWL15cM5302WefpWvXrlRWVnLYYYdx\n3nnn0a1btzrHWblyJS+//DJPPfUUF1xwAa+//jqXXHLJbufr3r07X375JY8++ij3338/Tz/9NL/7\n3e84/vjjufXWW3nvvfd45plnduu3cOFCNm7cyOLFJq65pMQs4X3FFVfw+OOPM3jwYObNm8dVV13F\nhx9+yJlnnlkjqNoSJzCSwPYyP/e9t4zy6hAL1u1gy64qzhnTh3PH9rGO4C5UB8OEVclKq/0XdMn0\nMbhnJ44d0oPV28q49bShbXgVjvZOVloKFf5Q3cKK7aBhKNvWNoNqhxx++OF1Yg8eeugh3nzzTQA2\nbNjAypUrdxMYAwcOZPTo0QAceuihrFu3Luaxzz333Jo2b7zxBgCffvppzfEnTpxIbm7ubv0OOOAA\n1qxZwzXXXMPpp5/OySefTFlZGXPnzuX888+vaef3+1t41ckhqQJDRCYCfwW8mCU2761XPwBjeuqB\nWRT9ElUtsHUh4BvbdL2qnpnMsSaKVdvKuOz5/7F1l58+ORn075bJwxeP2c3hGvEL1EdEeObSfKpt\nvIHD0RBZqSmUVwdR1dopk2GrcWi44Y57iMY0gT1JVlat32zOnDn8+9//5r///S+ZmZlMmDAhZlR6\nWlrthAyv10tlZWXMY0faeb1egsFgzDaxyM3N5euvv+b999/n8ccfZ9q0aTz44IPk5OTUaEftkWQ6\nvb3AI8BJmOC7L2yKj2+jmt2PifZ+QUSOB+4BfmTrKlV1dLLGl0gWfLeDG6ct5LiDevLW15sQEab9\n/EhG92t4tkVj+LyeRh3SDgcYDSOsUBUI176AhK3G0Q4ERluQnZ1NaWlpg/U7d+4kNzeXzMxMli1b\nxueff57wMYwfP55p06bx61//mg8++IAdO3bPalRUVERqairnnXceBx10EJdccgmdO3dm4MCBvPba\na5x//vmoKosWLWLUqFFNXteeIplPpcOBVaq6RlWrgVcw6UCiOQT40G7PjlHfrin3B/mmYCdX/n0B\nJRUBXvjvOlJTPEz7+REtFhaOVhAOQenWth7FHiMrzQiJ8uqoN9uIwAiHYvTo+HTr1o3x48czfPhw\nbr755t3qJ06cSDAYZOjQodxyyy0cccQRCR/DHXfcwQcffMDw4cN57bXX2H///cnOzq7TZuPGjUyY\nMIHRo0dzySWXcM899wDwj3/8g2eeeYZRo0YxbNgw/vnPfwIwadIk/vSnPzFmzBhWr16d8DHHjaom\n5QP8AGOGiuz/CHi4XpupwHV2+1xMssFudj8IzAc+B85u4BxX2Dbz+/fvr3uSfy3apIf89l0d8Ou3\ndehv39XlW3bp2sIyLSqtSu6Jd21WLdmQmGNVV6iunKW69VvVUGj3+uXvNX6ur19V/ehPqhXFZj8c\nrv370Z9UZ9+jGgrW7RMOqy54QXXxm42PLRw25y/dVlsWqNr9eBG2LVN98jjV33VV3fS1KassUZ37\niOquLaqf/U31nn6qDx+u+t5vVItW1fZd95nqo0ep/nmo6rwnGxwSMF+T9Htp7HPooYfGHM9r8zfo\ngF+/reuKymoL13ysekdn1S//3uB1JJNvv/22Tc7bnqiqqtJAIKCqqnPnztVRo0a18YhqifX/ac59\n3dZG8puAh0VkMvAxJso78mo0QFU3isgBwIci8o2q1hGt2kaR3tO+2MCvXl/EmP45TD4qj9H9chjQ\nrXnxBU1SXQ7znoCeQ6FLX6gsgbUfwdyHIbMrXPc1LJ0JnhQ4+Pvg8UAoCOvnQsEXkJIOfQ+HvvkQ\nqoaUNAiHYe0c4xBd+zEsfQv8dhZLRlcYfBKcNMVMydy0EKZeAB4fHDABOveCjFwYcQHsPxwK5sOb\nV4KG4JMH4MAToHCZcbrmHQ3fmjcjNn0Fh18B3QdD1S74/DFY+Hcz7rRO5jg7vgPxQEYOlBdBpx6m\n7ZcvwOBT4IfToLoCnj7BHHPSVNjwP0jLhl6joGg5vPpjSEmF1E7wwf+DiffCa5OhaAV8dB9UlUDe\nMeZ7mfcEfPkiXDEHug2CT/4MuzbBQadCbl6L/2Ui0g+TnXk/zMvPk6r613ptBOPXOw0TfzRZVb9s\nyfk6RTSMaMd3jQ9j39Qw2gPr16/nggsuIBwOk5qaylNPPdXWQ0oYyRQYDaX+qEFVN2E0C0SkE3Ce\nqpbYuo327xoRmQOMAdpQFzOs3FrK7TMXc/SB3Xlmcj5pKbGd102y9G3YMA+GnAKLpkG5zXm0dQmc\ncjfsWAv/+d3u/focChsXmIfcR/cZW3WvUfDD6fD6T41QiSatsxEKE++Dqp0w525b3gUOPgOGnQMV\nRbD2E/h2hvl7yXRYM8e0GzUJNi+EzV9D5Q6Y/xyccDvM/Rt07g3nPgWLXoEVH0BOP+i0nxEWoy+B\n/UfArN/Civfqjumoa+DbmfD38wAxAjEcNEIxsxuUbYVwAPYfCSvfh4IFsOhV2PYt+DLhoRiurZ6H\nwCWvm+/13ZvhsaOMEDzrUZj3OOQcDec/D14fFK+Fp46HVy+B856BVf+BY2+G4/9fy/6XtQSB/1PV\nL0UkG5Oaf5bW9dvFTOvfkpNlppqfb0W0SUr3bZNUe2Dw4MF89dVXbT2MpJBMgfEFMFhEBmIExSTg\n4ugGItIdKFbVMCb9x7O2PBeoUFW/bTMe+GMSx9oka4vK+flL81ldWE5Oho8HLhzVcmFRugVm/MI8\nyOc+BCkZ0PUA89AM+mHW7YAaDeGE282DOq0T7DfcPFD/Ohrm3GOEwUlT4N1fwyOHm3Yn3wVjf2S0\njaUzzYO+aAV8cJs59yFnw3H/D3IHGK0jwuiL4cir4KVz4L1bjAbQYyic9XBtm50F8ML34Z2bjGC4\n4CXoPw4GHFnbRhUKl0P3IUbrGfsjIxh3bjQP67yjjYAYcQF89lc48mroM5Y6VJaY76hLH3hwpDln\noByOuAqGnwfzn4VRF4HHa7SaQBWMvshoQPmXwaYvIac/HP5zyOoGY35Y9/hdB8IPnoG//8AIDoAx\nu8+xby6quhnYbLdLRWQpJtNytMCImdbf9m0WER9GmT+GD8NpGI4kkMxI76CI/BJ4HzOt9llVXSIi\nUzA2s5mY1CH3iIhiTFKRyO+hwBMiEsY45u+t95a2R1FVbpvxDZtLqrh6wiBOH9mbntn1chqFAvDp\ng9DtAPNQa4xZt0OwCia/AzvWGVNQp56mbulb5s0XYMKtMPCY3fsf9hP4951w9PXmAZmRY8wvh06G\no35Z2y7/MvO3ohgePwYCFXD6nyGrgQju/UfAEb+A/0wxpqjDLq9b36UvXD4LtnxjzDveGLePCPQ8\nuHY/NQsGHb97u14jzUM7Fhk55gNwyl1GuzjoNDj0MmN26ptf23bAUXX7en1wzuOxjxvNoOPhh6/B\ntEth8MlGgCYQuxbMGGBevaqG0vfXERjxZGLOSkuhM2X0XPIsDPm1+e5rTFIdPhenow1Iqg9DVd/B\n5IyKLrs9ans6MD1Gv7nAiGSOrTnMWLiRz1Zt5/dnDeNHR+bVrSyYD6tnw+oPjf9AvJDVAwYea/wQ\nnzxgbPFH/hKOuhZKN5kH4NE3QN5484nmoNNhvxGm3SENTBo7/AqjlRw62ewPOwf65EPnBpYNyewK\nP/uPEVINCYsIY34Ms+8xJqGB39u9Pqs7DDqu8WMkktEXm08yOPAE4wtKSW26bTOw5tXXgetVtUWh\nzvH457JSUzjX+ymHLHoRvne+8cfs47OkHMmlrZ3e7Z7FG3fymzcWM7Z/DhePi3oLVYUZV8HXU81+\nZjc4/QH435PwyiXm7fyb6bBzPfQcBv++w5hZetro7REXxD6hxwMXvQzVZXVNRtGkZsERV9Yty2ki\nY2n2/k1fLBiH87CzYcmbuwuzjkhWt6bbNAMR8WGExT9U9Y0YTZr07cVLVloKQ6TA7ARsYJlzejuS\nSFKjw0RkoogsF5FVInJLjPoBIvIfEVkkInNEpG9U3aUistJ+Lk3mOBtie5mfn704n9xMH4//6FC8\n0cn/dqw1wmLsj+HWAvjVGiMkLn7VmEw+fcA88C97F37xmREQX71kTE7ZvWsFRyxy+jVen2wm3mfM\nZeld2m4MeyF2BtQzwFJVfaCBZjOBH4vhCGxa/5acLzPVy4EeK2uCNlp5Hw/cawmdOpnMy5s2bWow\nV9OECROYP39+o8d58MEHqaioTQZ52mmn1eSI2lOsW7eOqVOnJu34SRMYUZHep2IC9C4Skfq5eSOR\n3iOBKZhIb0SkKyYd+jhMAOAdUetj7BFCYeX6VxeyvbyaJ3+cv7vPYqOdCXnYT830zgi5efCjN+CG\nJUZQDDjK2Jbzf2K0hlWzjP28PWcSzepmnNmO5jIeE290vIgstJ/TRORKEYmohO8AazBp/Z8Cmr9w\ngyXNK7trGG6WVIvp3bs306fvZiGPm/oC45133ml0bY1ksNcKDFoX6X0KMEtVi1V1BzALmJjEse7G\n1Hnf8cnKIn535jCG9+liTFDrPjVxBaGg+ZuSbqZzxqJL37ompf5HQFebkvjAGE5gx16Pqn6qqqKq\nI1V1tP28o6qPq1kDBhsrdbWqDlLVEara+GtrI0hFETlSbnZqNIx92yR1yy238Mgjj9Ts33nnndx/\n//2UlZVxwgknMHbsWEaMGFETQR3NunXrGD58OACVlZVMmjSJoUOHcs4559TJJfWLX/yC/Px8hg0b\nxh133AGYhIabNm3iuOOO47jjjI8vLy+PoqIiAB544AGGDx/O8OHDefDBB2vON3ToUH72s58xbNgw\nTj755Jg5q1577TWGDx/OqFGjOPbYYwGTHv3mm2/msMMOY+TIkTzxxBM11//JJ58wevRo/vKXv7T6\n+6xPMn0YsWaD1H9t/RoTh/FX4BwgW0S6NdC3AY9u4qkKhPjbh6s4fGBXJh1mzc0Lp8I/7cvgsHNM\nCor9R5hZOfEQ0TJm3w0H7EGnsaPjUrisdru+DyPcDkxS795iZtQlkv1HwKn3Nlh94YUXcv3113P1\n1WbC5bRp03j//fdJT0/nzTffpHPnzhQVFXHEEUdw5plnNrjO9WOPPUZmZiZLly5l0aJFjB1bO/X7\nrrvuomvXroRCIU444QQWLVrEtddeywMPPMDs2bPrrHsBsGDBAp577jnmzZuHqjJu3Di+973vkZub\nG1ca9SlTpvD+++/Tp0+fGjcZK0oAACAASURBVBPXM888Q5cuXfjiiy/w+/2MHz+ek08+mXvvvZf7\n77+ft99+u0Vfb1O0dYa7m4DvichXwPeoG+ndJCJyhYjMF5H5hYWFCRvU1Hnr2Vbq54YTh5gbqnKH\nmQrb9zAztXPJm7BxPvQe2/TBojniKrhhsZm15HC0lsLltdu7aRjtQGC0AWPGjGHbtm1s2rSJr7/+\nmtzcXPr164eq8pvf/IaRI0dy4oknsnHjRrZubTjv2Mcff1zz4B45ciQjR9auSzNt2jTGjh3LmDFj\nWLJkCd9+2/iM/08//ZRzzjmHrKwsOnXqxLnnnssnn3wCxJdGffz48UyePJmnnnqKUMg8Hj/44ANe\nfPFFRo8ezbhx49i+fTsrV65s1nfVEtplpLeIbMTEaET3nVP/BMlIDRIMhXnqkzWMG9iVIwfZGTT/\nfcSkvPjRG5DdCxb+w6TbqB9w1hQejxMWjsQRU8OwgqI9mKQa0QSSyfnnn8/06dPZsmULF154IWCS\n+hUWFrJgwQJ8Ph95eXkx05o3xdq1a7n//vv54osvyM3NZfLkyS06ToR40qg//vjjzJs3j3/9618c\neuihLFiwAFXlb3/7G6ecckqdtnPmzGnxWOIhmRpGTaS3iKRiIr1nRjcQke4iEhlDTaQ3JtjvZBHJ\ntc7uk21Z0vn30q1s3lnFjUNL4IUzTeT11iXGV9FrlAmwG2EXOGmuhuFwJJJty9jstZbaoF1op8Yk\n1Q4ERhtx4YUX8sorrzB9+vSaxYh27txJz5498fl8zJ49m++++67RYxx77LE1zuPFixezaNEiAHbt\n2kVWVhZdunRh69atvPvuuzV9GkpBfswxxzBjxgwqKiooLy/nzTff5JhjYgTkNsDq1asZN24cU6ZM\noUePHmzYsIFTTjmFxx57jEAgAMCKFSsoLy9Pehr0dhnprarFIvJ7jNABmKKqxckaazQvff4dfXIy\nOKx4usnLVLwWSjeb5HsRTrjDTJ3tPnhPDMnhiM2RV/P2fxbxs8J7IFg/DmPfNEkBDBs2jNLSUvr0\n6UOvXuZ3+8Mf/pDvf//7jBgxgvz8fA4++OBGj/GLX/yCyy67jKFDhzJ06FAOPfRQAEaNGsWYMWM4\n+OCD6devH+PH18YqXXHFFUycOJHevXsze/bsmvKxY8cyefJkDj/8cAB++tOfMmbMmAZX8avPzTff\nzMqVK1FVTjjhBEaNGsXIkSNZt24dY8eORVXp0aMHM2bMYOTIkXi9XkaNGsXkyZO54YYbmvPVNYlo\nB0khkJ+fr03Nk26K77aX870/zeHmk4dw9ZdnQNkWuOQNE6A3+EQ465GmD+LosIjIAlXNb7plYmns\n3r7+5fk8uPwEmPAbmPBr+PQvJm3M+OtMnrE9zNKlSxk61C0t3F6J9f9pzn3d1k7vdsWnq8wUuLN7\nFRthAVCyHsq3Gd+Fw9HOyEhLI0DK7hrGPmySciQPJzCi+GJtMT2y0+hd9Flt4eaFRr2PN7WGw7EH\n6ZTmxa8+k7EXXKS3I6m0dWqQ/iIyW0S+sulBTrPleSJSGRUtG0f60dbzxbodHJ7XFVn9oZnv3Wk/\ns/YEOA3D0S7JTE2hEh/hmllSbS8wOoqZu6ORiP9LW6cGuQ2YpqpjMLOoHo2qWx0VLVsv017iKdhR\nwcaSSg7LyzXrR/QabRYI2mrnWDuB4WiHZKV58ZNKyG9TUrSxSSo9PZ3t27c7odHOUFW2b99Oenp6\n040bIZlxGDWpQQBEJJIaJDrKRYHOdrsLsCmJ42mUL9aZSViH98uEWVshZ4AJ2NtkV85yAsPRDslK\nS6FKUwlVV+KDNk8N0rdvXwoKCkhkIK0jMaSnp9O3b9+mGzZCW6cGuRP4QESuAbKAE6PqBtoI8F3A\nbar6Sf0TxLPITLzMX7eD7LQUDkrfaQpy+tcumyoes8aFw9HOyEpNoYpUQtVWw4iYotrIJOXz+Rg4\ncGCbnNuRfNra6X0R8Lyq9gVOA16ygXybgf7WVHUjMFVEOtfvrKpPqmq+qub36NG6B/q3m3dxSO/O\neHetNwU5/Y1JCiCrZ+zV5RyONiYz1UsVqehuuaTcLClH4kmmwIhnoZjLgWkAqvpfIB3orqp+Vd1u\nyxcAq4EhyRpoOKws31LK0F6dzTRasALDRtG6GVKOdooxSfnQgMsl5Ug+bZoaBFgPnAAgIkMxAqNQ\nRHpYpzkicgAwGLOGQFLYsKOCw4PzObdsqhEYHp8REl0iAsP5Lxztk4iGQf1ZUk7DcCSBtk4N8n/A\nUyJyA8YBPllVVUSOBaaISAAIA1cmMzXI0s2lXJ/yBiNWrIHgBLOWhcdba5JyGoajnZKVlsJGUiFk\nfW9Ow3AkkaQa5lX1HcwKY9Flt0dtf4tZpax+v9cx6yLvETZ8t5qJntVmZ80cGGgWKSG7F6Rmu5xR\njnZLZqqZVuvZbYlWp2E4Eo/z5AKZa2eZDW8ahPzGfwFmxbyr50FW94Y7OxxtSFaqmVbrCVmB4ZZo\ndSSRdhnpbetutf2Wi8gp9fsmkkE7PqLQ1wuGnmEKcgbUVnbpU3epVYejHZGZZnwY3lC99OZOw3Ak\ngXYZ6W3bTQKGYdbyfjTiBE80VVVVjAl+Q0GP78GBJ5nCnNbFdDgce4pUr4dqScUbrj9LykVaOxJP\nMjWMmkhvVa0GIpHe0TQU6X0W8IqdXrsWWGWPl3A2r19BmgRgv2Fw8Okw9lIYdHwyTuVwJBwRIeRN\nw6shCAXdLClHUkmmwIgV6d2nXps7gUtEpADjHL+mGX0TQvEGsy5ydq+DIL0znPkQdHJR3Y69B/Xa\n/EDBSuf0diSV9hrpHRcicoWIzBeR+S3NXVO51cyO6jnALfri2DvRFCswAlVuWq0jqbTLSO84+yYk\nNYjuWEMVPjr3aF1SLoejrRBftIbhUoM4kke7jPS27SaJSJqIDMREev8vGYPMKF3PNm8v8LS1suVw\ntJCUDPM3UFVrinImKUcSaJeR3sASEZmGSYUeBK5WTc4vINe/kZ1Z/Zpu6HC0U8RnBUa0DyPsTFKO\nxNMuI71t3V3AXckcnz8QpFd4K0s7H53M0zgcSUVSozQM58NwJJF92g6zqeA7MsWPt5vL3+/Ye/Gm\nxtAwnEnKkQT2aYGxfcMyADr1crmiHHsv3rRMsxH0O6e3I6m0dWqQv4jIQvtZISIlUXWhqLr6zvKE\nULnNZEzv2uegZBze4dgj+FKNwNBAtIbhTFKOxJM0H0ZUapCTMIF3X4jITOu3AEBVb4hqfw0wJuoQ\nlao6OlnjA6guNymhu3R1gXqOvZeUdCMwAv4KUt0sKUcSaevUINFcBLycxPHsRmWVWXTG60vdk6d1\nOBJKmhUY1ZXlUSYpp2E4Ek9bpwYBQEQGAAOBD6OK020U9+cicnYD/VoV6e2vsgnbvC4brWPvxZee\nBRgNw2WrdSST9uL0ngRMrxdrMUBV84GLgQdFZFD9Tq2N9Pb7IwLD16JBOxztgfTMaIHhfBiO5NHW\nqUEiTKKeOUpVN9q/a4A51PVvJIRAdRUhPGY5VodjLyXdmqSC0QLDzZJyJIG2Tg2CiBwM5AL/jSrL\nFZE0u90dE9z3bf2+rUFVCVZXEfI4/4Vj7yYr3UeV+gj5K51JypFU2jo1CBhB8opNCRJhKPCEiIQx\nQu3e6NlViaC8OoQnHEA9zhzl2LvJTPVSjY+Qi/R2JJk2TQ1i9++M0W8uMCKZYysq9ZNKEPU6DcOR\nGETkWeAMYJuqDo9RPwH4J7DWFr2hqlNae96s1BT8+AgH/FFrejuB4Ug8SRUY7Znt5X58BMEJDEfi\neB54GHixkTafqOoZiTxpZpoXPz40UOVSgziSSnuO9L5URFbaz6WJHlthaTU+CSIpTmA4EoOqfgwU\n7+nzZqWm4Fcf6lKDOJJMu4z0FpGuwB1APibt+QLbd0eixldU5ieXAJ7IamUOx57hSBH5GrN+/U2q\nuqS1B8zweakmBV/Q76bVOpJKe430PgWYparFVkjMAiYmcnBFZX5SCZHiorwde44vMfFFo4C/ATMa\naticoFSPRwhIal0Nw5mkHEmgvUZ6x923pRSV+cn0OpOUY8+hqrtUtcxuvwP47LTxWG2bFZQa9qTi\nCUU7vZ3AcCSe9hzp3SStSQ1SVFpNpjfs0oI49hgisr+IiN0+HPP7256IYwc9qXhClbUFdWapOxyJ\nob1GesfVtzWpQbaX+8nwhFxaEEfCEJGXMQGoB4lIgYhcLiJXisiVtskPgMXWh/EQMKle/FGLCXvS\n8NURGE7DcCSeZE6rrYn0xjzsJ2HyQtUhVqQ3JtjvbhHJtfsnA7cmcnA7KwOke0KQ4jQMR2JQ1Yua\nqH8YM+028ef2puKrjhIYziTlSALtMtJbVYtF5PcYoQMwRVUTOl2xrCpIqovDcHQQNCWNtEgyTY/P\naRiOpNAuI71t+bPAs8kaW5k/iC8t4ExSjg6BpKSRppHsy6kQ8rftgBwdkvbi9N6jqCrl1SEb6e1M\nUo4OQEo6GVgh4fU5k5QjKeyTAqMqECYUVlLUaRiOjoEn2heXkgaomynlSDgNmqRE5MY4+per6hON\nHGMi8FeMD+NpVb03RpsLgDsxEd1fq+rFtjwEfGObrVfVM+MYT1yU+U1wU4oGndPb0SHwpEZlLIj4\n5cIh8O6z6eIcSaCxu+lm4DFAGmlzJRBTYMSTGkREBmNmP41X1R0i0jPqEJWqOjq+y2ge5VZgeDXg\nnN6ODoHXFy0wrNbs0oM4EkxjAuOlplIvi0hWI9U1qUFs20hqkOh1LX4GPBLJEaWq2+IadSuJaBie\ncLUzSTk6BCl1NAyrNbuZUo4E06APQ1V/1VTnJtrEk95jCDBERD4Tkc+tCStCuo3i/lxEzo51gpZG\nehuBoXjCAef0dnQIUtIyanciL0HO8e1IMHE7vUXkCBF5T0TmiMg5CTp/CjAYmIBJPviUiOTYugGq\nmo8J9ntQRAbV79zSSO9yf5AUQgjqTFKODoGvjsCw97QzSTkSTIMCQ0T2r1d0I3AOcBoQzyph8aT3\nKABmqmpAVdcCKzACBFXdaP+uAeZgU58ngjJ/0EypBXDJBx0dgNS0zNqdFGeSciSHxjSMx0XkdhGJ\nGEdLMLlwzgF2xXHsmtQgIpKKieieWa/NDIx2gc3aOQRYIyK5IpIWVT6eur6PllNdzqh5N9JXisy+\n0zAcHYDUtFizpJyG4Ugsjfkwzga+At4WkR8D1wNpQDcgpk+hXv8gEEkNshSYFkkNIiKRKbLvA9tF\n5FtgNnCzqm4HhgLzbZK22cC90bOrWsW2ZeRtfo+jPHbdGuf0dnQA0jKiNIwak5TTMByJpdFJ2qr6\nloi8A1wFvAncZZehjIumUoPY/FE32k90m7nAiHjP0yyCJn1CrpSafef0dnQAMtKjBYabVutIDo35\nMM4UkdnAe8Bi4ELgLBF5JZYDeq8haDJ69vCWm31nknJ0AFLTo0xSER+GmyXlSDCN+TD+AJwKXADc\np6olqvp/wG+Bu+I5uIhMFJHlIrJKRG5poM0FIvKtiCwRkalR5ZeKyEr7uTT+S2qCoMm3071GYDiT\nlGPvR1JizZJyAsORWBozSe0EzgUygZqAOlVdiXFgN0prIr1FpCtwB5CPSRmywPbd0czr2x1rkuoq\nZWbfpQZxdASiZ/s5k5QjSTSmYZyDcXCnEGPhozioifRW1WogEukdTUOR3qcAs1S12NbNAiaSCKyG\nkRMRGM4k5egIpMSI9HYmKUeCaVDDUNUi4G+tOHasSO9x9doMARCRzzAJCu9U1fca6Fs/SrxlWA2j\ni0ac3k5gODoA0fexC9xzJInGnN5fNtU5njZN0Fikd5O0KDVIwAiM7LATGI4ORLSGkRKVrdbhSCCN\n+TCGisiiRuoF6NJIfbyR3vNUNQCsFZFIpPdGbEBfVN859U+gqk8CTwLk5+fHl/zfahgZWmH2ncBw\ndASifXFOw3AkicYExsFx9G/sFaYm0hsjACaxuy9kBkazeC460htYDdwtIrm23ckY53jrCdZbutKl\nBnF0BOoIjIjT22kYjsTSmA/ju9YcWFWDIhKJ9PYCz0YivYH5qjrT1p1sI71D1EZ6IyK/xwgdgCmq\nWtya8dRgNYwanIbh6AhEB6A6p7cjSSR1Oa6WRnrbumeBZxM9pmCgsu5FO4Hh6Ah4fSiCoKjXZ1Y9\ncyYpR4LZ59b0DlZV1i1wAsPRERAh6DGaRRAXh+FIDk0KDBG5JsqXsNcTqnYCw9ExCXuMoPCr1aGd\nScqRYOLRMPbDRGlPs6k+Glvjuw5NpQYRkckiUigiC+3np1F1oajy+mnRW8xuAsM5vR0dhLD1XfjV\nawqc09uRYJr0YajqbSLyW8xMpcuAh0VkGvCMqq5uqF88qUEsr6rqL2McolJVR8d7IfESCjint6Nj\novZerqoRGM4k5UgscfkwrHN6i/0EgVxguoj8sZFu8aQG2eOoExiOjooN3qsMO5OUIznE48O4TkQW\nAH8EPgNGqOovgEOB8xrpGm96j/NEZJGITBeR6EC/dBvF/bmIxFywqSWR3hqIMkmJFzzeuPo5HO0e\n+/JTGXYmKUdyiEfD6Aqcq6qnqOprNiobVQ0DZ7Ty/G8Beao6EpNg8IWougGqmo8J9nsw1hocqvqk\nquaran6PHj3iO2PQT1itG8ZpF44OhEQ0jJAzSTmSQzwC412gJmhORDqLyDgAVV3aSL8mU4Oo6nZV\njYReP43RWiJ1G+3fNZi0IGPiGGvTBP2UkGW2ncPb0YEQnxEYFSH7s3ZrejsSTDwC4zGgLGq/zJY1\nRU1qEBFJxaQGqTPbSUR6Re2eiVn7GxHJFZE0u90dGA8kZE1vT8hPsXY2O07DcHQgPD4zS6rCmaQc\nSSKeSG+xTm/AmKJEJJ7ZVfGkBrlWRM7EONKLgcm2+1DgCREJY4TavTFmV7UIT8jPTskFNrn1vB0d\nCk9qBiEVKgK2wDm9HQkmHoGxRkSupVaruAqTILBJ4kgNcisxkgqq6lxgRDznaC7ecBVl3s4Qxi3P\n6uhQeFPSqMZLRdC+3zkfhiPBxGOSuhI4CuN/iCyCdEUyB5VMUsLVlHntkhvOJOXoQIgvnTAeKgMR\ngeE0DEdiice0tI041vDeW0jRaipTukAA5/R2dCy8qYTES7kzSTmSRDxxGOkicrWIPCoiz0Y+8Ry8\nlalBLhWRlfZzafMuqwFCQVIIgS8DUjs5DcPRsUjvgl/SnUnKkTTi8WG8BCwDTgGmAD/EzmZqjNak\nBhGRrsAdQD6gwALbd0cc422YkJnB6/GlQzjLCQxHx2L8dUxZPIisaicwHMkhHh/Ggar6W6BcVV8A\nTsf4MZqiNalBTgFmqWqxFRKzgIlx9m0YmxbE4zQMRxKw2vc2EVncQL2IyENW414kImMTOoBOPdmU\ndTDlER+GM0k5Ekw8AiNiES0RkeGYdbx7xtGvNalB4urb7NQgdrU9b2o6ZHaFtOw4LsPhiJvnafzF\n5lTMmvWDMRNH4olnahaZqSmUOae3I0nEIzCetOth3IYJvPsWuC9B528sNUiTNDc1iFqBkZKWCWc/\nBqfc3YIhOxyxUdWPicqKEIOzgBfV8DmQUy94tdVkpXmpqLamKGeSciSYRn0YIuIBdlmz0MfAAc04\ndlypQaJ2n8YkOIz0nVCv75xmnDsmlZUVZAIpaRnQfXBrD+dwNJeGNOfNiTpBZmoKBc4k5UgSjWoY\nNsHgr1p47BanBsFEh59sU4TkYtbieL+F46ihsqIcAF9aZmsP5XAklZZkYgbISvVS6pzejiQRzyyp\nf4vITcCrQHmkUFUbU71blRpEVYtF5PcYoQMwpanzxUNEYKSmZ7T2UA5HS2hS646gqk8CTwLk5+dr\nrDaxyExLobxawYfTMBwJJx6BcaH9e3VUmRKHeaqlqUFs3bNAXPEe8VJVVQFAWrrTMBxtwkzglyLy\nCmam4U5VTZg5CoyGURWyAsNpGI4EE0+k98A9MZA9QXWlFRgZWW08EkdHRERexvjeuotIASaWyAeg\nqo9jXp5OA1YBFZgljxNKZmoK4Yil2c2SciSYJgWGiPw4VrmqvhhH34nAXzEmqadV9d4G2p0HTAcO\nU9X5IpKH8Wcst00+V9UrmzpfU/j9RmBkOA3DkQRU9aIm6pW6mnrCyUrzEooIDGeSciSYeExSh0Vt\npwMnAF8CjQqMeCO9RSQbuA6YV+8Qq1V1dBzji5tAlVmeNSPTaRiOjkldDcOZpByJJR6T1DXR+yKS\ng4naboqaSG/bLxLpXT81yO8xcR03xzPg1hD0G6d3ZlanZJ/K4WgTstK8ziTlSBrxBO7VpxyIx6/R\nZLS2TY3QT1X/FaP/QBH5SkQ+EpFjYp2guVMPg9UmcC/TaRiODkpmakqUScppGI7EEo8P4y3MrCgw\nAuYQYFprT2yDAh+gdpW9aDYD/VV1u4gcCswQkWGquiu6UXOnHgarjUkqNc1Nq3V0TLJSUwgjZsdp\nGI4EE48P4/6o7SDwnaoWxNGvqTnn2cBwYI6IAOwPzBSRM1V1PuAHUNUFIrIaGALMj+O8DRK2GgYp\n6a05jMPRbumUngIIiiDOh+FIMPEIjPXAZlWtAhCRDBHJU9V1TfSrifTGCIpJwMWRSlXdCXSP7IvI\nHOAmO0uqB1CsqiEROQCTrC2uZWEbIxyoopoUUj0tscQ5HO2f3Eyz7LCKF3GzpBwJJp4n52uYFbAj\nhGxZo6hqEIhEei8FpkUivW10d2McCywSkYWY6bZXJiLSm2AlAXEpzR0dl87pPjyCcXw7k5QjwcSj\nYaTY9SwAUNVqmxuqSZqK9K5XPiFq+3Xg9XjO0SwCVQSdwHB0YDweoUuGj7CKm1brSDjxaBiF0RqB\niJwFFCVvSMlDglUEPGltPQyHI6nkZqYaDcPNknIkmHg0jCuBf4jIw3a/AIgZ/d3e8YT8hFKcwHB0\nbHIyfYTKnUnKkXia1DBUdbWqHoGZTnuIqh6lqqviObiITBSR5XZJylsaaXeeiKiI5EeV3Wr7LReR\nU+I5X1N4w1WEvG6GlKNjk5uZamIxnNPbkWCaFBgicreI5KhqmaqW2TUq/hBHv0hqkFMxwuYiETkk\nRrvdUoPYdpOAYZglLx+1x2sxqkpK2I+6KbWODk5OZioh58NwJIF4fBinqmpJZMeuvndaHP1qUoNY\np3kkNUh9IqlBqqLKzgJeUVW/qq7FZPc8PI5zNkhlIEQa1U5gODo8uZk+KzCchuFILPEIDK+I1Bj+\nRSQDiMcR0JrUIE32tf3jTg1SWhUknWoXtOfo8ORmpRLEQygUbOuhODoY8QiMfwD/EZHLReRyYBZN\nZKqNh6jUIP/X0mOo6pOqmq+q+T169Gi0rREYAcTn0oI4OjZdMnyEEaoDTmA4Eks82WrvE5GvgRNt\n0e9VNZ71tVucGiSOvs2mzB+kG9V4Ut1aGI6OTWRabXUgiHs9ciSSuHJkqOp7qnqTqt4ElIvII3F0\nq0kNYgP9JmGWqIwcc6eqdlfVPFXNAz4HInmkZgKTRCTNphYZDPyveZdWl9KqAOlSjTfV/YQcHRvj\nw/BQHQi09VAcHYx44jAQkTHARcAFwFrgjab6qGpQRCKpQbzAs5HUIMB8VZ3ZSN8lIjINs3ZGELha\ntXUevLKqIGlUE3SZah0dnBw7rTbgBIYjwTQoMERkCEZIXISJ7H4VEFU9Lt6DtzQ1iN2/C7gr3nM1\nRWlVkAyqqUxzJilHxyY3y0cFQiDoZkk5EktjGsYy4BPgjEignojcsEdGlQTKKqvwSYhQuls8ydGx\nyc1MpRQPGnROb0diacyHcS5mIaPZIvKUiJwAkZVZ4qOpSG8RuVJEvhGRhSLyaSSwT0TyRKTSli8U\nkcebc95YVFaUAZDqBIajg5Pu86LiJRR0JilHYmlQw1DVGZiV7rIwgXTXAz1F5DHgTVX9oLEDR0V6\nn4SJo/hCRGaqavSa3lNV9XHb/kzMNNuJtm61qo5u4XXthr/SrOftcU5vxz6AeDwEXByGI8HEk0uq\nXFWnqur3MdNbvwJ+Hcexm4z0rrfkaha1S8EmHH9lhdlwgXuOfQDxeAk6H4YjwTRr6TlV3WGD5U6I\no3m80dpX2yVY/whcG1U1UES+EpGPROSYWCdoTqR3wG8Fhgvcc+wDeL0pBJwPw5Fg2nytUlV9RFUH\nYbSW22zxZqC/qo4BbgSmikjnGH3jjvSurjImKadhOPYFwqnZZAf2ymVr2o4598Kq/7T1KNo1yRQY\nzY3WfgU4G8AmHdxutxcAq4EhrRlMsDqiYTiB4ej4FPUYx0F8R3nxprYeSvuicgcEqmLXffZX+Gb6\nnh3PXkYyBUajkd4AIjI4avd0YKUt7xFJZy4iB2Aivde0ZjAhf6XZSHEmKUfHx59nwqV2LW4gi0/Z\nNvj0QdCkuQ3bJ0+fBB/du3t50A+BCqgs3vNj2otImsBQ1SAQifReCkyLRHpHLfn6SxFZIiILMaan\nS235scAiWz4duFJVW/WfDFdbgeE0DMc+QKcBYynSzsjqD2M3WDID/n0HlHy3ZwfWloQCsH0V7Fi3\ne12lXcGhYvseHdLeRlypQVpKU5HeqnpdA/1eB15P5FjC1RVGPDoNw7EP0LdrFh+HR3Lapk/M23P9\npYkjb9JVu3bv3FEp2waoMUvVJ1JW4TSMxmhzp/eeIBAKI0Frt3QahmMfoEenNN7XI0kP7IAnjjWm\nmFcvqW0QeTD69yWBscX8bUxgOJNUo+wTAmN7WTXpYqNenYbh2AfweITlXcbzWO+7zdreRcth+Xu1\nPot9UcMojUdglLi10BshqQKjpalBbN2ttt9yETmlNeMoLPWb1fbAaRiOfYY+uRm8HxgN18yHY38F\n4QBURWz1EYGxs+0GGIsVH8D855Jz7NLN5m9ljGuuESJa689w7EbSBEZUapBTgUOAi6IFgmWqqo6w\nKUD+iEkNgm03CRiGSRXyaGTWVEsoKosSGE7DcOwj9M3JZGOJneyRZeOUym1sRuQB2d5MUv97Ej75\nc+1+KAC7Nifm2KVbZ9Cm0AAAIABJREFUzV//TqifNiVa63BmqQZJpobRmtQgZwGv2HiMtcAqe7wW\nUVjqJ02qUWR355/D0UHpk5tBYamfqkAIOkUEhs2I0F5NUmVbjFCLmM4WPA8PH2Yc962lNErw1Nes\nogVGe5kptWszFCyo3S8rhK9fadOp0MkUGK1JDRJv37hSgxRGNIyUdJBmJdx1OPZa+nU12nTBjspa\nDaNsm/lbEdEw2plJqmwbBCuh2mZmKFwG1aW1gq41RHwYUGuai7XfXmZKzb4LXjobwmEj0F48C978\nOWxf3WZDanOndwOpQeLtG1dqkMJSP9neIOL8F459iAO6dwJgdWFZlEmq0JhjIoKiPWkY4VCtYKiw\nprNdNlK9PAFpTkq3gDfVbNd3fFfuqK1rrYYx+x746E/N7xcOwZz7zKy2qp2w7VtjMty5HmZeA9uW\nmHZFy5s+VuFy+OC2hAu/dpkapAV9G6WozE+OL+T8F459igN6mLVfVheWQWZ3U1heVPdtuj35MMoL\nQcN2OyIw7M8+EWaisi3QzSaXiCUwuh5gt1vxkN2yGD66Dz7+ozEhNYc3r4Q5d8Pmr2HjAihcYY/5\nDaz6EEZOMvuFTQiMBS/AY+Nh7t/gq783/xoaoV2mBrHtJolImogMxKQG+V9LB1JU5qdzStDNkHLs\nU2Sn+9ivcxqrt5WDNwUyupqHcvRbZ3vSMKJNRuX1NIzWCoxQwFx7z6FmP5bA6NIXPL7WvZX/53eQ\nmgWhauN/qY+/rK62tOIDeOt6WP85fDMN8i+vLa8uNdtLZkCgHAYdD9m9oWhFw+ff8D/4142QdzT0\nGArL3m75tcSgXaYGUdUlwDTgW+A94GpVbfHk6MJSP528AadhOJJOHFPJJ4tIYdRqkj9N5ngG9ehk\nNAyATj2hfFvtG7R425eGUba1dru80Di6Iyaq1pqkIsfuebD5W3/qbOUOI1Azu7ZcOG1ZDCs/gGNv\nMg/3L56GhVPrpiJ5eRL8aRA8OcGM4dMHYMFz8NI5kJELJ94JXfrD4qhEF0vfMn/7HArdB9cKjFAQ\nFk2rdeAH/TD9J9C5D5z/PAw/DzbMM0J345ctu6Z6JNWHoarvqOoQVR2kqnfZsttVdabdvk5Vh6nq\naFU9zgqKSN+7bL+DVPXdFg8i6Kdn6VKyPE7DcCSXOKeSA7xq7/nRqvp0Msd0YE8jMFTV+DHKi2rf\noLv0SZyG8c10aChvVbxEC4yKoroaR0UrBUZkSm2PRjSMjFzI7BY7sA9g6dvw2uSGZyktexsQGP1D\nOPZmqC6DGb+Av46Cly8yDv11n0L/I2HTV0ZQbJgHOQNM4sOjroH0ztBrpBHsAL1GQ8gP6V2MyazH\nQcZUpQpL3oA3fgZTJ0Gg0giZnRvgjL9ARg4M/b45xpPHwTMnQdGq1nyDQDtweiebwBfP8TK30DNQ\n4DQMR7Jpcir5nmZQj06UVgUpLPNDVnfzxh55IObmJS5wb9bt8N6trTtG5KHuTTOCbVdUavbWahiR\nJIs5/SE123wH21ebVOfhkPkeMnKMltGQhrH4dVjyJhQ3kDh72dvQb5zR5AYcBb/+Dq6aB+OuhOXv\nwKw7AIWJ90D3IcbBrWE490n4yfsw/vr/3955h1dVZY37XekFSEISeiK9iFKDVBVQmgoofZAZQBkU\nmbGM429wiu3Dn84HY0FHbDAqVkBQLAioIChlSDD0EgOhhBZChySk7O+PvU9yc03gJuSSwn6f5z73\n3rP2Pmefc9c9+6y191pb76dOG/0eGAZNdNZh6nUAHx9d78IZ3ZkmvANB4bBvDXw8Gtb8W3eITXrr\nOtEtoPZ1usO548WCMZrLoLwjvf8kIttEZJOIfCci17jIcl3M9kXudT3lZIS++GGZqdbCsHgbj6aD\nA0ONzs8XkZgi5CVaTfJiNIk2M6WOnoPQWnog1nFJRTQsG5dUdqa+uaftgKM7Sr+fs4f1U3612qbD\nMAPevgGXP4Zx8Ge9n+iW+hiHNur4jpfbaNcRGAujZvFjGEeMAyRlle48fpqhO4+P74Ylf9OD0y1v\nLyjv66ddYLc+pW/sGz+E6nW11XD9CD19ODgCGnSC2C7gY2KT67bV79HNoVZr/blBnH6PMssC7fgS\n9v4E3R+Cwa/C7hVwZAt0ub8gdEAExn4BD22CDr/THc5l4rVstS7meR/0H2e9iCxSSm1zKfYzEKeU\nOi8ik9CxGCONLMNEgF8Wh4ObUkP5EyjZdrU9S0XgC+AjpVSWiNwHvAv0di+klHoTeBMgLi6u1JFa\nTWoVzJTqGhqtp9OeOQw+flCjgXaF5GaDr39pDwEn95Efc7t9UcE4QUk5cxiq1dHLKJ9LK7AwarW6\nfAsjdYN+2vYL0JbEvtV6e3AELP5LweeQmkXPksrOhHQzJ2fXUu1ayjoFy/6hB8rzTK461w7DwT8Y\n2o+BNa9Cs776Rn79MFg+VVsDPm5JLOoaCyOqhe4ofAOgsbE0nA5j2RP6N2x3N1Svrd2NWxZAm5GF\n9xVSs2TX6RKUd6T3cqWUWQqPtejps2XK0fOKrcoYLnY9b4t3ueR0cKVUulLKCVt+G+jozQbVqRFE\n9SA/th86rV1SoAdNgyO0Xxwg68zlHcQZ1A0Mg22fl34/Z49qd05otB6zOH1Qu48iGl2ehZGXC4cS\n9aAx6HMH3WGOmFNQLjgCwmJ0Z/XLt4X3kbZDu48Cw2DnV7qz6PU3uH44PLAW7pypxy0imxTdhk73\nandXmxH6e81Guk7PItx41etCx3HQZrguN2UfNOxuZHXg2sHQ6GYYOkt3FgDN+8GQN7x+jyv3SG8X\n7gVcB7eDjEm+VkTuLKqCJ2b7sbNZJOY11V+shWHxLp5MJa/r8nUQegah1xAR2sWEs2HfSX0zBpcO\no4b+7jqOkZ4Mp0oY8nRij37v+DvtFilp/IHD2cP6hhgaVeCSqlFXfy9q0HvrQpg94NID98d26QHo\n+h309+Bw/d64p3b7XGsmbQaFQ+f7tCUy7x5Y90bBuIrjjmpvUsTXulZ3EEPfhqim0G409L5I3HHN\nxvCXPXq6q0O70XrWkzsiMPBl3T4o3AmIwIj3YPTH0LrI26JXqRCD3iIyBogDXMMjr1FKxQGjgZdE\n5FddtyeR3j1b1KLzjX31F2thWLyIh1PJHzRTyTeiU+GM83a72sdGsPPwac77G/fEib36aTfQdBjO\nOEZ2BvxnAHz5cMkOcCIF/EMhtpv+fnJfyRuplL45V6vt0mEchBr1dNBhxonCCQOzM+Gbv2rXkpOs\nUCl9Du44U0rdLYzGPfV7r79D8/7a9RVYHX7zkY7JWPz/4M2bdTuObNUPnHH36OnIne+7KtMMeXPF\nPY+itUXkVuBvwM0upjpKqVTzvltEVgDtgRInUaldI4janXrDGqyFYfE6Hqwy+ThwmdOJSkaH2HDy\nFGzMiaVrx/GQtAxiO7tYGKbD+Pl9PbX1yNbid1YUx/foAfTwWP391D5oUEJP27k0PZunel3Iy9Gf\nD2+CDmMLXGkZx/V4yxcPanfamYN6RtHa1+DgBj2QnXka2o6CW57QU2TXztSWSED1gijvkEj93ugm\n/R7dHEZ/UtCW8Fh4YDUciNf5mz4apd1RtVppa+LhTTrW4SrEmx1GvnmO7ihGoa2FfESkPfAG0F8p\nddRlewRw3gwMRgHd0QPipSOioZ6V0LhnqXdhsVRW2sfoJ+qE1PN0HfhSgeBgon7POq1vxD+9rL+f\nTtU3XqdDuRQnUrTvPtw8H5bGwnCsgLptC+r7+EOPh3X0MmirI/m7gvGF2K46QO3tPjoIrvUQPRC8\n4T3YuVi3KTVBW1NtRxbMEoq7B+q1L/D/F0eDOO1ymn+PnhzQ4Xd6e1iZD7VWGrzWYSilckTEMc99\ngdmOeQ7Em+C9aUA1YJ5o826fUmoQ0Ap4Q0Ty0G6z591mV5UMERj61uWdkMVSSQkL8adprWp6HMMV\nZ9A78zRs/EgHfXUcp+f3H0vyzEpQSncYTW/R+wsKg5MuQ5cnUvQD26VIjdeunnrtdFoNgJ5/0Tdn\nJ+7h/DHYswpqNoG7XtcBb9VrwyObC++ryySYPx4ObYIhb+vB40IXpIHnN/0WA+DPu/S01fpenZ9Q\nKfCmheGJeX5rMfVWA9d7s20Wy9VEx9gIvtl6mNw8ha+P8b07Hcb5Y3rhonodoNuDusNI2+FZh3H2\niI4ncDqF8NgCC2HTPFgwAX67sCCYrDgOxOuB5IBQ7Soa9ZGeggoFLqSzR2Hvat0BxFxkeZzIJjDh\nOx1PcSkrwhMCqxdETV/lVIhBb4vF4l26NY3kVEY2Ww+6zIhyBr1XTtc3+Z6P66d230DdYRTF3tU6\nFYUT3JZqFvhxppOGxWpL5fxx+MbE6m5ZcPHG5eVpl5QTnObjCy1v04FvoAfCATa8q6OcG9546RP2\n9S+bzsJSCNthWCxXAd2b6oHjVUku01N9/WDAND3dtM1IaNZHb4tqVnQK7bxc+PJPsGuxzoiqlB5U\nrtGg4CbuWBgrp+mZTfU66LQYBxL0+gy52ToXUvLygv2mJ+m4BqfDcCc0Sk9n3bNSf/ekw7B4Ba+6\npESkP/AyegzjbaXU827yPwETgBwgDbhHKbXXyMZSsKDSVKXUu95sq8VSlYmqFkjrejVYuSuNyb2a\nFgg6T9QvV6JbaBeRO5s+gbTtOmhs60I9wJyyCvo+WxApHh6jYx42zNFxAq0Gwbyx8O5AnaLbNwAS\nP9IznOLugT7PuGRjLabDALjtXzrtiMorWG7WcsWpkKlBRKQm8CQ6NkMBCaZuMWkkLRbLpejRLIrZ\nP+7hbFYO1QIv8tePbqndSBfO6TEF0FbB0r/r2UVjFuhYjcQPtFvLmT0EBVNrL5zRMxMb9tDT2fNy\ndM6kVf/Sg9ttR0P8bNj8qbYuGvcsSHtRFP5BMP5rbaFYyo2KmhqkH7BMKXXcdBLLgP5ebKvFUuW5\nqVk02bmK1b9cIi9TzA2A0tNX962FT34L79yub/RDZ2m31eBX4Y8JcO+ywtNvnQ4jOEIPdAdWg/7P\nw/D/6LohkdDrr3DXTPj99zrpXtc/wN3zL50czy9Q789SbnjTJVVUapDOFynvmhrEo7QiIjIRmAgQ\nGxt7OW21WKo8nRrWpEaQH99sOUzf1nWKL3hNDx1dvWmuzsCadVqny+j/XOFcSUWly3Y6jNZ36UR/\nAHHjC+SP7ixwX9XvCHfPvbyTslxRvDqG4SkuqUFuLkm9ssroabFcDQT4+dC3dR2WbDlMVk4ugX6+\nRRf09dMJ7uJn6e+jPiw6C2tRBEfAyPd1UF2R+76MrLiWcsebLqmSpgYZ5JIaxKO6FoulZNzepi5n\nsnL4MekSbqnrhuj36JbQfEDJDtJqYEE6D0uVwpsdhieZO53UIINcU4Ogo8P7ikiESRPS12yzWCyX\nQfcmUdQI8mPBz5d4/ortqlNt9H22TBbesVQNKmRqEKXUcRH5H3SnA/CMUqqYZbAqJ9nZ2Rw4cIDM\nzMzyborFjaCgIBo0aIC/f9VznwT4+XB3l2uYuSKZ8d2OE9ewmAV2fHz1QLXF4oKo4hY0r2TExcWp\n+Pgi5o5XUPbs2UP16tWJjIxErsI0yRUVpRTp6emcOXOGRo0aFZKJSIJJuX9FKWvdPn8hh1v/9QPV\ng/z56sEe+PlaC+JqpiR6bTWlnMjMzLSdRQVERIiMjKzSll9IgB9PDGzNziNn+GBdKTLLWq5avNph\niEh/EdkpIr+IyJQi5DeJyAYRyRGRYW6yXBFJNK9F7nWrArazqJhcDb9Lv9a16dYkkheW7eLEuQvl\n3RxLJcFrHYZLpPcA4FrgNyJyrVuxfegVxz4sYhcZSql25jWoCLnFYiklIsITA6/lTGY2b/+4u7yb\nY6kklHekd4pSahOQ58V2WIrg5MmTvPbaa6Wqe9ttt3Hy5MmLlnniiSf49ttvS7X/y+Gzzz5j27bS\nL51yNdGyTg1ubh7Ngg2p5OZVjbFMi3fxZofhUbT2RQgSkXgRWSsiRa52LiITTZn4tLRSLjx/lXKx\nDiMnJ6fI7Q5ff/014eHhFy3zzDPPcOutRS534lVsh1EyhnZswKFTmaxJTi/vplgqARUi0rsYrlFK\npYpIY+B7EdmslCq0pndVifR++outbDt4ukz3eW29Gjw5sHWx8ilTppCcnEy7du3o06cPt99+O//4\nxz+IiIhgx44d7Nq1izvvvJP9+/eTmZnJQw89xMSJOqtpw4YNiY+P5+zZswwYMIAePXqwevVq6tev\nz+eff05wcDDjxo3jjjvuYNiwYTRs2JCxY8fyxRdfkJ2dzbx582jZsiVpaWmMHj2agwcP0rVrV5Yt\nW0ZCQgJRUQVBX7m5udx7773Ex8cjItxzzz088sgjJCcnM3nyZNLS0ggJCeGtt97i+PHjLFq0iB9+\n+IGpU6fy6aef0qRJk+IugQW4tVVtqgf5MfunPdSPCKZRVGh5N8lSgSn3SO/iUEqlmvfdwAqgfVk2\n7mrn+eefp0mTJiQmJjJt2jQANmzYwMsvv8yuXbsAmD17NgkJCcTHxzNjxgzS03/9FJqUlMTkyZPZ\nunUr4eHhfPrpp0UeLyoqig0bNjBp0iSmT58OwNNPP03v3r3ZunUrw4YNY9++X8/YSUxMJDU1lS1b\ntrB582bGj9d5iSZOnMgrr7xCQkIC06dP54EHHqBbt24MGjSIadOmkZiYaDsLDwjy92V051i+33GU\nXtNX8NWmQ+XdJEsFxpsWRn6kN7qjGAWM9qSiie4+r5TKEpEooDs69XmV5GKWwJXkhhtuKBR7MGPG\nDBYuXAjA/v37SUpKIjIyslCdRo0a0a5dOwA6duxISkpKkfseMmRIfpkFC/QKbD/++GP+/vv3709E\nRMSv6jVu3Jjdu3fzxz/+kdtvv52+ffty9uxZVq9ezfDhBWs1Z2Vl/aquxTOm9G/Jne3q8+jcjfz/\nr7dzS6taBPkXk2fKclXjNQtDKZUDOJHe24G5TqS3iAwCEJFOInIAGA68ISJbTfVWQLyIbASWA8+7\nraNh8QKhoQXuiBUrVvDtt9+yZs0aNm7cSPv27YuMTQgMDMz/7OvrW+z4h1PuYmWKIiIigo0bN9Kz\nZ09ef/11JkyYQF5eHuHh4SQmJua/tm/f7vE+LYUREVrVrcHfb29F6skMXv4uiaoS0GspW7wah6GU\n+lop1Vwp1UQp9azZ9oRJC4JSar1SqoFSKlQpFamUam22r1ZKXa+UamveZ3mznVcj1atX58yZM8XK\nT506RUREBCEhIezYsYO1a9eWeRu6d+/O3Lk6vfXSpUs5ceLX62MdO3aMvLw8hg4dytSpU9mwYQM1\natSgUaNGzJs3D9DR2Rs3bvTovCzF061pFEPa12fmimTum5PAV5sOkZ1rJzBaCrCR3lcpkZGRdO/e\nneuuu47HHnvsV/L+/fuTk5NDq1atmDJlCl26dCnzNjz55JMsXbqU6667jnnz5lGnTh2qV69eqExq\naio9e/akXbt2jBkzhueeew6ADz74gFmzZtG2bVtat27N559/DsCoUaOYNm0a7du3Jzk5+VfHtFyc\n6cPb8li/FqxKOsbkDzfw1CJt9B85XXUj3y2eY3NJlRPbt2+nVatW5d2MciUrKwtfX1/8/PxYs2YN\nkyZNIjExsbybBRT9+1SVXFKekJ2bx5OLtvLJ+v38tss1vLM6hRdGtGVIhwaXrmypVFSYXFKXmRpk\nrIgkmddYb7bTUj7s27ePTp060bZtWx588EHeeuut8m6SxeDv68Of+jQnyM+Hd1an4OcjTF+yk8zs\n3PJumqUc8dosKZfUIH3QQXvrRWSR2+C1kxrkz251awJPolfhU0CCqftrJ7el0tKsWTN+/vnn8m6G\npRiiqgXyxMBrWZOczuB29Rn/znreWZ3C/Tfb6cpXK96cVpufGgRARJzUIPkdhlIqxcjcR9b6Acuc\nNTBEZBnQH/jIi+21WCxujOwUy8hOep3uPtfWZtqSndQNC6JeeDCt69UgJMDzW0hunuLxBZu4u/M1\ntI25eKYAS8WkoqYG8aiuTQ1isVw5XhzZjtb1avDQx4kMf30NA1/5kV+Oej4jbXPqKebGH+DNVTbZ\nYWWlUs+SUkq9qZSKU0rFRUdHl3dzLJYqTbVAP+bc25mXRrbjhRFtOXk+m6Ez17Dz8BnOZeWQc4kp\nuD/9otcR/377UTIu2LGQyog3XVKXkxokFejpVndFmbTKYrGUmrBgf+5sr439Tg1rMuz11QyduZrM\n7FwaRAQzpss1fLx+P7e0rMWjfVsQ4FfwTLomOZ0gfx8ysnNZsfMoA66vW16nYSkl3rQw8lODiEgA\nOjWIpwshLQH6ikiESRPS12yzlCPVqlUD4ODBgwwbNqzIMj179uRSU0Bfeuklzp8/n//dk3TpZU1K\nSgoffljUMiwWT4mpGcL793ambUwY47o15EJOHlO/2k5mdi5vrNzNyDfXcPzcBT77OZWVu9JYn3Kc\nkXExRIYGsGjjwfJuvqUUeM3CUErliIiTGsQXmO2kBgHilVKLRKQTsBCIAAaKyNNKqdZKqeMi8j/o\nTgfgGWcA3FL+1KtXj/nz55e6/ksvvcSYMWMICQkBdLr0K43TYYwe7VF6M0sxNKtdnQ8m6KDOP/Zu\nxsYDJ+nRNIrFWw7zyNxEevzze867uJ9ubBZNUIAvb/ywm1e+S+IPvZteFSscVhW8mt5cKfU18LXb\ntidcPq9Hu5uKqjsbmO3N9lUYFk+Bw5vLdp91rocBzxcrnjJlCjExMUyePBmAp556imrVqnH//fcz\nePBgTpw4QXZ2NlOnTmXw4ELrXpGSksIdd9zBli1byMjIYPz48WzcuJGWLVuSkZGRX27SpEmsX7+e\njIwMhg0bxtNPP82MGTM4ePAgvXr1IioqiuXLl+enS4+KiuKFF15g9mz9s0+YMIGHH36YlJSUYtOo\nuzJv3jyefvppfH19CQsLY+XKleTm5jJlyhRWrFhBVlYWkydP5r777mPKlCls376ddu3aMXbsWB55\n5JGyuvJXLWEh/tzUXI8l3t6mLhEh/kz9ajujO8eybs9x1iSn07lxTXq2iCbtdBb/WraLbYdO82jf\n5jSOqoaPT9EdR+rJDCa9n8CQ9vUZ171RkWUsV4aKvB6GxYuMHDmShx9+OL/DmDt3LkuWLCEoKIiF\nCxdSo0YNjh07RpcuXRg0aFCxT4EzZ84kJCSE7du3s2nTJjp06JAve/bZZ6lZsya5ubnccsstbNq0\niQcffJAXXniB5cuXF1r3AiAhIYH//Oc/rFu3DqUUnTt35uabbyYiIoKkpCQ++ugj3nrrLUaMGMGn\nn37KmDFjCtV/5plnWLJkCfXr1893cc2aNYuwsDDWr19PVlYW3bt3p2/fvjz//PNMnz6dL7/8siwv\nq8WFbk2j+PqhGwEY0+UacvMUvqZTmD68LU1rV+OlZUks3nKY8BB/br++LvuOn2fn4TOIwD3dG9Gr\nZS0e/jiRbYdOs+nAKY6eyeLPfVsU6lyUUqzZnc7W1NME+PkwtGMDqgUWf2uLT9Gd1+ReTYvtpCxF\n49UOQ0T6Ay+jXVJvK6Wed5MHAu8BHYF0YKRSKkVEGqIz3O40Rdcqpe73ZlvLlYtYAt6iffv2HD16\nlIMHD5KWlkZERAQxMTFkZ2fz17/+lZUrV+Lj40NqaipHjhyhTp06Re5n5cqVPPjggwC0adOGNm3a\n5Mvmzp3Lm2++SU5ODocOHWLbtm2F5O78+OOP3HXXXflZc4cMGcKqVasYNGiQR2nUu3fvzrhx4xgx\nYkR+OvWlS5eyadOmfBfaqVOnSEpKIiAgoOQXzXJZ+LrcnH18hAd6NmVgm3qs2Z3Oyl1pzI3fT/3w\nYHq2iObQqUyeW7yD5xbvwEdg1tg4vt1+hNdWJLPryFmmDWvDoVOZfLBuL2uS09l97Fz+vt9cuZt2\nMeHk5il+f1Mjlm07Sm5eHo/2bcGa3encPyeBrJw8cvIUj/RpXqiNObl5nMrIJrJaYP73vcfP0ygy\n1KPORSnlkYvtTGY2d722Gj8fYURcDPf0qByWU3lHet8LnFBKNRWRUcA/gZFGlqyUauet9llg+PDh\nzJ8/n8OHDzNypL7sH3zwAWlpaSQkJODv70/Dhg2LTGt+Kfbs2cP06dNZv349ERERjBs3rlT7cXBP\no+7q+nJ4/fXXWbduHV999RUdO3YkISEBpRSvvPIK/fr1K1R2xYoVpW6LpeyIqRlCTM0QRsTFcCEn\nD39fyb/hrklOJ+1sFi1qV6dFner0blmL5rWr8+xX2+n1rxWczcwh0M+HDtdE8ECvpvRpVZuko2f4\nx+db2Zx6itOZ2Xyz9TAioBTMSzjAyfPZtKxTnaa1qvHyd0kcPJlBcIAvG/efpFFUKOtTTpB6MoPY\nmiE0rVWNHYdOc/BUJrVrBPK7rg0ZHteA/cczaF2vBvuPn+eDdfuIrh5IctpZ4lNOcPhUJmO6XMNv\nu15DyrFz5ClF7RpBNIoKJSTAlwu5efj5+PC/3+wkOe0sbeqH8cyX22gQEUzf1kU/lLly/NwFnvli\nK/2vq0v/64ovr5Qi6ehZagT5U7tGYJmNE5VrpLf5/pT5PB94VewI2BVj5MiR/P73v+fYsWP88MMP\ngH4Cr1WrFv7+/ixfvpy9e/dedB833XQTH374Ib1792bLli1s2rQJgNOnTxMaGkpYWBhHjhxh8eLF\n9OzZEyhIQe7ukrrxxhsZN24cU6ZMQSnFwoULmTNnjsfnk5ycTOfOnencuTOLFy9m//799OvXj5kz\nZ9K7d2/8/f3ZtWsX9evX91oa9NJa1WXekEqI6xRcgK5NCi/WJSKM796ILo0jeeaLbcTUDOavt7Ui\nPKTAWoxrWJPFxg124twFPt1wgM6NIjl2Nos5a/fSo2kUIzrF4OcjhAb48dXmQ+Tk5dGmfjg//pJO\nk+hQRneOZfOBU+w9fp4mtaox8abGLN+ZxrQlO5m2RDs96ocHc/L8BS7k5pGdqwgP8adr40jaxYQz\n+6c9zP5pz6+HBLsYAAARFklEQVTOz0cgT0Ggnw9ZOXnc070RUwa0ZPC/f+KvC7fw7fYjBPj50DAy\nFKWgbngQ57JyWJV0jKEdGxAW7M+fPkkkJf08X20+xMy7O9K7ZS22HTrN15sPsTvtHKM7x1IzNICX\nv0ti2bYjANQNC6Jf6zr8oXdToqoF/qpdJcGbHUZR0dqdiytjZlWdAhwtaSQiPwOngb8rpVa5H0BE\nJgITAWJjY8u29VcBrVu35syZM9SvX5+6dfWc+LvvvpuBAwdy/fXXExcXR8uWLS+6j0mTJjF+/Hha\ntWpFq1at6NixIwBt27alffv2tGzZkpiYGLp3755fZ+LEifTv35969eqxfPny/O0dOnRg3Lhx3HDD\nDYAe9G7fvn2xq/i589hjj5GUpBf/ueWWW2jbti1t2rQhJSWFDh06oJQiOjqazz77jDZt2uDr60vb\ntm0ZN25cmQx6l4FVbfGAVnVr8NHES6fbjwgNYMKNjfO/92pZq5D8n8Pa8D93XkeeUpdcYXBc90as\n3Z3Oz/tOUicskFk/7qFOWBD/Ht2B0EBfQgL88l1uo26IIeXYeVrUqYavjw+pJzLYf+I8ZzKzCfb3\n5cT5bE5nZPNo3+YE+PkwfXgbRr2xlhU708jIzuVMZuEFxkICfPnSLJ0bXT2Qd++5gee+3s6E9+IJ\n8PPhQk4evj5CWLA/32w9DOjO99E+zQkL8WdV0jEWbDjAn/u1uPTFvQReS29uss/2V0pNMN9/C3RW\nSv3BpcwWU+aA+Z6M7lTOANWUUuki0hH4DGitlDpd3PFsenNLWVKa9OYi0hV4SinVz3x/HEAp9ZxL\nmSWmzBoR8QMOA9HqIn/EyqbblpLjjH0opTiVkY2Pj7D/+Hny8qB5nWq8t1pb+nd3iSUkwE+727Yc\nZtvB01xXP4xeLaKpFuTH4s2H8fMVOsRGUC+8YBbhhZy8X1lwDiVJb17ekd5OmQPmzxMGpJs/TxaA\nUirBdCTNAfuvsVRkLseqPuZayFrPVxeOJ15E8l1sreuF5ct/f1PjQuVrBPkzIi4Gd5wofHeK6yxK\nSnlHei8CnLUuhgHfK6WUiEQb8x4RaQw0A2zGMstVg82TZqmIlGukNzALmCMivwDH0Z0KwE3AMyKS\nDeQB91fFSG9Pp+BZriyX4aYttVVd2gNaLFeS8o70zgSGF1HvU+BTb7atvAkKCiI9PZ3IyEjbaVQg\nlFKkp6cTFBRUmur5VjW6YxgFuOcecazqNbhY1ZfRZIvlimEjvcuJBg0acODAAew6HhWPoKAgGjQo\n+drVl2lVWywVHtthlBP+/v40alQ5ojstnlNaq9piqQx4dQElEekvIjtF5BcRmVKEPFBEPjHydSYl\niCN73GzfKSL93OtaLBaL5critQ7DJYhpAHAt8BsRudatWH4QE/AiOogJU24U0Bq9lvdrzqwpi8Vi\nsZQP3rQw8lODKKUuAE5qEFcGA++az/OBW0xqkMHAx0qpLKXUHuAXsz+LxWKxlBMVNTVIfWCtW91f\nRaS4BjcBZ0Vkp3sZQxRugVFWZmWlkF1TTHmvkpCQcExEikvqVVGujZVVXpnneq2U8soLPWXwbZfv\nvwVedSuzBWjg8j3ZnNCrwBiX7bOAYZfRlngrs7KykFW0V0W6NlZWuWWevLzpkipJEBNuQUye1LVY\nLBbLFaRCpgYx20eZWVSN0KlB/uvFtlosFovlElTI1CCm3Fz02hk5wGSlVG6RB/KMN63MyspIVtGo\nSNfGyiq37JJ4Lb25xWKxWKoWXg3cs1gsFkvVwXYYFovFYvGMy5liVRle6EjxnejgvylushRgM5CI\nnpt8FNjiIq8JLANOAReAbS6yp9Azt7aiVwjcZz4/5FJ3JXAeOAtsd5E5dc8BGcBe4Gkja4Qe4M8E\nTqLHcRzZO8Ae097zwEqXOuvMOc418i+LqHMBSDKf493OMducx2YXmdPOzaYt+815dHWplwwcAXa5\nyJ4y2zLMKxm91O7Dpt5P6AWynOM5Mud4qaZeCvAREGTOca+pdwr4xGx3zu+Ay7Eedju3NJff4GFg\ntrn22eZaJAIjTNkk8x5h9iHADHNtNwEdylunL6XXZaTbh83vk2mu+5XS60RgBwX664luXzDH20nl\n1Ot1aB09hb6HODLn/DzV7bXo3/moOVaiaccGykivq7SF4WF6kl5KqXbAEPSf0JUpwHfAQOB1wH0l\nmxeBW4GblVKxQBdgsjnGFGAV0AOYCnzjInPq1lZKBQNNgf4i0gWdHuUFdDzKx6b9jgzgMeA99LK1\nzpK1/wReVDrFSgP0H8iVx8w5HgS6KqXaqYIlGZ1zTDVt+loVXq7xRbTC/VkpFQO0RSunU+9H81ro\nIgOYppQKNufXHH2DWWjqfaGUCgSeBRa7yEBPhLgA1FRKNURPmBiFVu5gIBz9h2pKQabXGeg/fiTQ\nArhDRJqaY21B/8n/CXwB3IH+47wPHDPXoh0QB3ynlGpmzsvJfTYAPUuvGTpIdCbljId6DZen22+g\n9ToIuI4roNemre+h9c3hkrqN1usYpVSLSqjX/0R3DGfRHeKrLjIomW77AROc8zPXczEwt6z0ukp3\nGHiWngQApdRK9EwtVwYD7xrZHKBGEfUOKaU2mM9n0IpV39R91cjeBW5zkTl1z5qP/ualgN7AfCN7\nF7jTRQb6yeJ24G0Ak0qlNzBfRBqYsiVZYMM1Pcsn5niuBKEXtJpl2nxBKXXS1FtgZH8A7nSRuXML\nkKyU2ut2vHeB37jIHPyAYBObEwIcArqjn96C0Tf7GPSNAvSNZJ1S6rxSKgf4AX2THIx+kl2Hvl6D\njSwG/cRW3HVwrruz/T2lWQuEi0jdIs7xSuKxXkOpdfvMFdZrjP6WlW5XFr3uDXxlZPOAu4ysNLr9\nOdoSKu46XLZeV/UOo6j0JK4pRhSwVEQSTJoRd2orpQ6Zz2n8ehryH0Rkk4jMFpEIk223PfpHdK17\nGKjrInOtm442IR0z+KTSU5J90YrQG1imlHLq/QuIBSahf79Ipw7wEvAk+inOlWdFZBMQASxzO1+n\nnQp942jidi0eAGoDu0x73xaRULMtxFyX50w9R1bo2gC/Q1sF7tf0MFDLRYYp64N+cjqMNtMT0De8\n6WjX3+dAgFJqqakzBBgjIjNFJBx9E4sxbfwRuBH9dFfbRQYQ6dLGOm7tqm0+X0qHygNP2lSWut0G\n7+v1s+gn5sPoJ2zwULeBesBWEdlQCfX6pFJqH1q3F5rrcuoydNu56f/B/Ocbot2ATrsuS6+reodx\nKXoopTqgzbPJlCzB4UygCdAO/aTwMnqVwIeVUqfdyoaildCRudZ9He1eugFo6VRQOu7E8VPfICLX\noU3KOWgXQXWzDwBE5A70H3Sz27EfN/vtBCxFK+UAtBvhpiKuxVkX2Uz0E0kAWjnj0f5px6z1AzqY\ncmdcZK7ndwQdlDmviGvoPIE6splAR3POr6LN7FBzHXzQT0WN0O4jHxEZY86vMfAgehzC8YPnmuu4\nHW2yLzX7cmTvo/3Tzu+Xv8Se0k7eyj7fvKx0+xjwPd7V68eBR9GWUg766TwfD3S7Bdo1tozKp9eI\nSARat7uhx29CL0O389DZMpx2KvRDZpnodVXvMC6aYkQplWrej1Lgq3TliIuZFo1WZqfuEaVUrlIq\nD/gP2pT8QCm1wLWuiPijFeS4I3Or+xb6h12ONifDjcnqtHe/kfVH+6sHYf5saB/xy2i/fg8j+y/6\niau3iLxvXGZKKZWF9hvf4HK+NzjtVEqlmnM94siUUkfQT/QHgKdN+fnoP9MR9JPNAVPmqCNzO7/d\nQLbZl/s1HQ1kODLz3hs90PcyupNbgHZHRQEpSqk0oA56QLWby/m9AQxFP6WeQHcGzrnNQo9d7HaR\nHTPHdH4DnHaZ96OmjRUxTc0l21QWuo1+0u+M/v28ptfmybw7+jfqh9aL3niu23vQ/8HWVD69Dgf6\nGlmI+R0XUHrd3g3kurQzFd0RlYleV/UOo9j0JCISKiLVnc/oH22XW3331CX5T1guNxdBm54HlVIv\nFFF3Fvpp4x3XuiISbUzMu9A+4D7mfTkw3sjGoldv64N+8pihlGqAfsr+HtijlLrb1Ek0ss+B19Bp\nVsa4tDMUvdLbFpfz3WLaOcFcC+d4fU25ukqpw+g/90RT/ha073QR2gTej346/NyRuflCJ5nyRV3T\nR9A+Wddrug89eWCE2/HWA31EJMTUPwdsdzm/Wuinxn1oM/5D51hGNtZcM0fmOsh7F/qP5rRrrDkf\np72/E00XtLvgEOXLRdPulIVuG72eZTb9UES9MtNro2ePo29gH6NvmN97otsu53on+kGqsun1cvTA\ncxf0+kDO8Uqr24so7F50ZhZCWei1qgBTBL35Mj/+LrQf9W8u2xsDG81rq3k/hJ6Gd8D8eJFoN9AZ\nc9FdZXPQJvIvaDNvGwUm422mbryROdPsHNkc9DS3DLQvcwfwhEu7NpvjnTRtc2TfG9kWtPm92KXO\nf01b5qH/iF+61dmJfgLZbPb5NyOPpPB0wB0uMuccd1EwFfIz9FiIc232om82W11kTr3N6Ke15i7X\n3an3i5Fd4yJz6h1xacscINCcY6qpcwr4wGx3zu+caeNm4Ba3Y2WgXRLOH/Uj9AwWZfaXiLbevjO/\ny7fo2SygB1n/jdafzUBceev0xfS6DHU72VyfU+a6XSm9fh/tRvvSpd7FdHuHOd4JdMdUGfX6v+gx\nOud4jqykuv29+Z3zzO+5Hz1LahVlpNc2NYjFYrFYPKKqu6QsFovFUkbYDsNisVgsHmE7DIvFYrF4\nhO0wLBaLxeIRtsOwWCwWi0fYDuMKIiK5IpLo8ppy6Voe77uhiGzxoNxTInLezN92tp29WJ2yboOl\namH1+urBa0u0WookQ+kMkuXNMXRQ0l/KuyGuiIif0nmDLJULq9cXoSrptbUwKgAikiIi/ysim0Xk\nv6LTFztPNt+LTnb2nYjEmu21RWShiGw0r25mV74i8paIbBWRpSISXMwhZwMjRaSmWzsKPUmJyJ9F\n5CnzeYWIvCgi8SKyXUQ6icgCEUkSkakuu/ETkQ9MmfkmMhsR6SgiP4hOhrfEJYp1hYi8JCLxwEOX\nfzUtFQWr11VPr22HcWUJdjPdR7rITimlrkcnJ3vJbHsFnYK6DTqyeYbZPgP4QSnVFp3/ZqvZ3gz4\nt1KqNToydGgx7TiL/nOVVJEvKL2mwOvoFAOT0YkQx4lIpCnTAnhNKdUKHbn6gOi8Q68Aw5RSHc2x\nn3XZb4BSKk4p9a8StsdSMbB6fZXotXVJXVkuZrp/5PL+ovncFZ0fBnS6gP81n3uj0yU72T9Pic54\nuUcplWjKJKBTGxfHDCBRRKaXoP1OvqLNwFZlcs+IyG50HqCTwH6l1E+m3PvoTJvfoP+Ay0QEdFI7\n17w1n5SgDZaKh9Xrq0SvbYdRcVDFfC4JWS6fc9GLDRV9MKVOisiH6KcphxwKW51BhWvl7z/P7Vh5\nFOiSe9sVOm/NVqWU++IuDueKa6el0mP1ugphXVIVh5Eu72vM59UULNV4NzqJGOikY5NAL9cpImGl\nPOYLwH0U/CmOALVEJFJEAtFpk0tKrIg4f6DR6PUGdgLRznYR8ReR1qVss6VyYfW6CmE7jCuLu6/3\neRdZhOgVsh5Cp0cG+CM6JfQm4LcU+GYfAnqJyGa0iV7Ues6XRCl1DL1GQKD5ng08g86euQydVbOk\n7EQvVLMdneFzptLLiA4D/ikiG9HZTbtdZB+WyoXV66tEr2222gqAiKSg0wsfK++2WCxlhdXrqoe1\nMCwWi8XiEdbCsFgsFotHWAvDYrFYLB5hOwyLxWKxeITtMCwWi8XiEbbDsFgsFotH2A7DYrFYLB7x\nf+fp866/nJooAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSoaYACAd1t",
        "colab_type": "code",
        "outputId": "4e4ba26e-08ac-4564-e2a4-7a9818c9fc45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# FOR MAKING PREDICTIONS ON TEST SET AND SAVING TO CSV\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import shutil\n",
        "from torch import nn, optim\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# number of times we iterate over the training set\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 672\n",
        "TRAIN_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_x'\n",
        "TEST_PATH = '/content/drive/My Drive/McGill/comp551/data/test_max_x'\n",
        "TARGETS_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_y.csv'\n",
        "THRESH = 240\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('torch.cuda.device_count(): {0}'.format(torch.cuda.device_count()))\n",
        "    print('torch.cuda.get_device_name: {0}'.format(torch.cuda.get_device_name(0)))\n",
        "    # load images as a numpy array\n",
        "    X_train = np.array(np.load(TRAIN_PATH, allow_pickle=True))\n",
        "    X_train = np.array([cv2.threshold(i, THRESH, 255, cv2.THRESH_BINARY)[1] for i in X_train])\n",
        "    X_train = X_train / 255.0\n",
        "\n",
        "    X_test = np.array(np.load(TEST_PATH, allow_pickle=True))\n",
        "    X_test = np.array([cv2.threshold(i, THRESH, 255, cv2.THRESH_BINARY)[1] for i in X_test])\n",
        "    X_test = X_test / 255.0\n",
        "\n",
        "    y_train = pd.read_csv(TARGETS_PATH, delimiter=',', skipinitialspace=True)\n",
        "    y_train = y_train.to_numpy()\n",
        "    # remove id column\n",
        "    y_train = y_train[:, 1]\n",
        "    y_train = y_train.astype(int)\n",
        "\n",
        "    # Clean memory\n",
        "    len_train_dataset = len(X_train)\n",
        "\n",
        "    X_train = torch.from_numpy(X_train)\n",
        "    y_train = torch.from_numpy(y_train)\n",
        "    X_test = torch.from_numpy(X_test)\n",
        "    y_train = y_train.long()\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=BATCH_SIZE, num_workers=8)\n",
        "\n",
        "    # defining the model\n",
        "    model = Net().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    time0 = time()\n",
        "\n",
        "    for e in range(EPOCHS):\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # clearing the Gradients of the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prediction for training\n",
        "        output_train = model(inputs)\n",
        "        \n",
        "        # computing the training loss\n",
        "        loss_train = criterion(output_train, labels)\n",
        "\n",
        "        # computing the updated weights of all the model parameters\n",
        "        loss_train.backward()\n",
        "\n",
        "        # And optimizes its weights here\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_train.item()\n",
        "      else:\n",
        "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss / len_train_dataset))\n",
        "\n",
        "    print(\"\\nTraining Time (in minutes) =\", (time() - time0) / 60)\n",
        "\n",
        "    # prediction for test set\n",
        "    predictions = np.array([])\n",
        "    with torch.no_grad():\n",
        "      for images in test_loader:\n",
        "        images = images.to(device)\n",
        "        output = model(images)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        predictions = np.append(predictions, predicted.cpu().numpy())\n",
        "\n",
        "    predictions = predictions.astype(int)\n",
        "    Id = list(range(0, 10000))\n",
        "    Id = np.asarray(Id)\n",
        "    results = pd.DataFrame({'Id': Id, 'Label': predictions}, columns=['Id', 'Label'])\n",
        "    from google.colab import files\n",
        "    results.to_csv('predictions.csv', index=False)\n",
        "    shutil.move(\"/content/predictions.csv\", \"/content/drive/My Drive/McGill/comp551/predictions.csv\")\n",
        "    print('Results have been saved to predictions.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.cuda.device_count(): 1\n",
            "torch.cuda.get_device_name: Tesla P100-PCIE-16GB\n",
            "Epoch 0 - Training loss: 0.0032704003858566285\n",
            "Epoch 1 - Training loss: 0.0029089065623283386\n",
            "Epoch 2 - Training loss: 0.002827518105506897\n",
            "Epoch 3 - Training loss: 0.0024329508543014526\n",
            "Epoch 4 - Training loss: 0.002178996772766113\n",
            "Epoch 5 - Training loss: 0.0019570931363105774\n",
            "Epoch 6 - Training loss: 0.0015498412847518921\n",
            "Epoch 7 - Training loss: 0.0011921281003952025\n",
            "Epoch 8 - Training loss: 0.0009742726159095764\n",
            "Epoch 9 - Training loss: 0.000797192497253418\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}