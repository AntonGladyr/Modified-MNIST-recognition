{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final solution",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbakhit/Modified-MNIST/blob/master/Final_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nJ47ABzStdv",
        "colab_type": "code",
        "outputId": "2df8ac15-4115-4a29-de37-d5d8defda308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUsWBmaBg52B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Net, self).__init__()\n",
        "        # Defining a 2D convolution layer\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "        self.drop_out = nn.Dropout()\n",
        "\n",
        "        # Defining another 2D convolution layer\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(1*1*256, 4096)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(4096, 256)\n",
        "        )\n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-TGbFWTQ49",
        "colab_type": "code",
        "outputId": "83bd27a6-55ce-4e6d-885d-1dc0780e828f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# FOR TESTING ON VALIDATION DATASET\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import inspect\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score, accuracy_score\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# number of times we iterate over the training set\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 256\n",
        "TRAIN_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_x'\n",
        "TEST_PATH = '/content/drive/My Drive/McGill/comp551/data/test_max_x'\n",
        "TARGETS_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_y.csv'\n",
        "THRESH = 240\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def calculate_metric(metric_fn, true_y, pred_y):\n",
        "    # multi class problems need to have averaging method\n",
        "    if \"average\" in inspect.getfullargspec(metric_fn).args:\n",
        "        return metric_fn(true_y, pred_y, average=\"macro\")\n",
        "    else:\n",
        "        return metric_fn(true_y, pred_y)\n",
        "    \n",
        "def print_scores(r, a, batch_size):\n",
        "    # just an utility printing function\n",
        "    for name, scores in zip((\"recall\", \"accuracy\"), (r, a)):\n",
        "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('torch.cuda.device_count(): {0}'.format(torch.cuda.device_count()))\n",
        "    print('torch.cuda.get_device_name: {0}'.format(torch.cuda.get_device_name(0)))\n",
        "    # load images as a numpy array\n",
        "    train_dataset = np.array(np.load(TRAIN_PATH, allow_pickle=True))\n",
        "    train_dataset = np.array([cv2.threshold(i, THRESH, 255, cv2.THRESH_BINARY)[1] for i in train_dataset])\n",
        "    train_dataset = train_dataset / 255.0\n",
        "\n",
        "    targets = pd.read_csv(TARGETS_PATH, delimiter=',', skipinitialspace=True)\n",
        "    targets = targets.to_numpy()\n",
        "    # remove id column\n",
        "    targets = targets[:, 1]\n",
        "    targets = targets.astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(train_dataset, targets, test_size=0.2, random_state=42)\n",
        "    # Clean memory\n",
        "    len_train_dataset = len(X_train)\n",
        "    train_dataset = None\n",
        "    targets = None\n",
        "\n",
        "    X_train = torch.from_numpy(X_train)\n",
        "    y_train = torch.from_numpy(y_train)\n",
        "    X_test = torch.from_numpy(X_test)\n",
        "    y_test = torch.from_numpy(y_test)\n",
        "    y_train = y_train.long()\n",
        "    y_test = y_test.long()\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test,y_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "\n",
        "    # defining the model\n",
        "    model = Net().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    time0 = time()\n",
        "    batches = len(train_loader)\n",
        "    val_batches = len(test_loader)\n",
        "    train_losses_arr = []\n",
        "    val_losses_arr = []\n",
        "    train_acc_arr = []\n",
        "    val_acc_arr = []\n",
        "\n",
        "    for e in range(EPOCHS):\n",
        "      running_loss = 0.0\n",
        "\n",
        "      # set model to training\n",
        "      model.train()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # clearing the Gradients of the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prediction for training\n",
        "        output_train = model(inputs)\n",
        "        \n",
        "        # computing the training loss\n",
        "        loss_train = criterion(output_train, labels)\n",
        "\n",
        "        # computing the updated weights of all the model parameters\n",
        "        loss_train.backward()\n",
        "\n",
        "        # And optimizes its weights here\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_train.item()\n",
        "\n",
        "        _, predicted = torch.max(output_train.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "      # releasing unnecessary memory in GPU\n",
        "      if torch.cuda.is_available():\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      # ----------------- VALIDATION  ----------------- \n",
        "      val_loss = 0\n",
        "      recall, accuracy = [], []\n",
        "      \n",
        "      # set model to evaluating (testing)\n",
        "      model.eval()\n",
        "\n",
        "      # prediction for validation set\n",
        "      # correct = 0\n",
        "      # total = 0\n",
        "      with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            output = model(images)\n",
        "            val_loss += criterion(output, labels)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            # total += labels.size(0)\n",
        "            # correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            for acc, metric in zip((recall, accuracy), \n",
        "                                   (recall_score, accuracy_score)):\n",
        "                acc.append(\n",
        "                    calculate_metric(metric, labels.cpu(), predicted.cpu())\n",
        "                )\n",
        "      # print(\"Epoch {} - Training loss: {} - Validation accuracy: {}\".format(e, running_loss / len_train_dataset, 100 * correct / total))\n",
        "      train_acc = correct/total\n",
        "      val_acc = sum(accuracy)/len(accuracy)\n",
        "      print(f\"Epoch {e+1}/{EPOCHS}, training accuarcy: {train_acc}, validation accuarcy: {val_acc}\")\n",
        "      print(f\"Epoch {e+1}/{EPOCHS}, training loss: {running_loss/batches}, validation loss: {val_loss/val_batches}\")\n",
        "      print_scores(recall, accuracy, val_batches)\n",
        "      train_losses_arr.append(running_loss/batches) # for plotting learning curve\n",
        "      val_losses_arr.append(val_loss/val_batches)\n",
        "      train_acc_arr.append(train_acc)\n",
        "      val_acc_arr.append(val_acc)\n",
        "\n",
        "    print(\"\\nTraining Time (in minutes) =\", (time() - time0) / 60)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_acc_arr)\n",
        "    plt.plot(val_acc_arr)\n",
        "    plt.title('Model accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['training set', 'validation set'], loc='upper right')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_losses_arr)\n",
        "    plt.plot(val_losses_arr)\n",
        "    plt.title('Model loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['training set', 'validation set'], loc='upper right')\n",
        "\n",
        "    plt.show()\n",
        "    # # accuracy on validation set\n",
        "    # print(\"\\nModel Accuracy =\", (100 * correct / total))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.cuda.device_count(): 1\n",
            "torch.cuda.get_device_name: Tesla P100-PCIE-16GB\n",
            "Epoch 1/40, training accuarcy: 0.2494, validation accuarcy: 0.2693359375\n",
            "Epoch 1/40, training loss: 2.1155142935977618, validation loss: 1.9252588748931885\n",
            "\t        recall: 0.1157\n",
            "\t      accuracy: 0.2693\n",
            "Epoch 2/40, training accuarcy: 0.265625, validation accuarcy: 0.26953125\n",
            "Epoch 2/40, training loss: 1.9440160701229314, validation loss: 1.8953428268432617\n",
            "\t        recall: 0.1145\n",
            "\t      accuracy: 0.2695\n",
            "Epoch 3/40, training accuarcy: 0.358825, validation accuarcy: 0.41669921875\n",
            "Epoch 3/40, training loss: 1.6999739021252676, validation loss: 1.6018399000167847\n",
            "\t        recall: 0.2424\n",
            "\t      accuracy: 0.4167\n",
            "Epoch 4/40, training accuarcy: 0.46295, validation accuarcy: 0.46533203125\n",
            "Epoch 4/40, training loss: 1.447628875446927, validation loss: 1.5497452020645142\n",
            "\t        recall: 0.2876\n",
            "\t      accuracy: 0.4653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/40, training accuarcy: 0.58045, validation accuarcy: 0.62939453125\n",
            "Epoch 5/40, training loss: 1.1888303692173805, validation loss: 1.050848126411438\n",
            "\t        recall: 0.3767\n",
            "\t      accuracy: 0.6294\n",
            "Epoch 6/40, training accuarcy: 0.692, validation accuarcy: 0.7216796875\n",
            "Epoch 6/40, training loss: 0.8981641212086768, validation loss: 0.7895535826683044\n",
            "\t        recall: 0.5023\n",
            "\t      accuracy: 0.7217\n",
            "Epoch 7/40, training accuarcy: 0.765075, validation accuarcy: 0.73095703125\n",
            "Epoch 7/40, training loss: 0.6927239031169066, validation loss: 0.9256048202514648\n",
            "\t        recall: 0.4880\n",
            "\t      accuracy: 0.7310\n",
            "Epoch 8/40, training accuarcy: 0.8105, validation accuarcy: 0.63203125\n",
            "Epoch 8/40, training loss: 0.5707212827008241, validation loss: 1.1244386434555054\n",
            "\t        recall: 0.5036\n",
            "\t      accuracy: 0.6320\n",
            "Epoch 9/40, training accuarcy: 0.832, validation accuarcy: 0.82470703125\n",
            "Epoch 9/40, training loss: 0.509477229444844, validation loss: 0.5245811343193054\n",
            "\t        recall: 0.6186\n",
            "\t      accuracy: 0.8247\n",
            "Epoch 10/40, training accuarcy: 0.857325, validation accuarcy: 0.83994140625\n",
            "Epoch 10/40, training loss: 0.4306861230522204, validation loss: 0.5798912048339844\n",
            "\t        recall: 0.6456\n",
            "\t      accuracy: 0.8399\n",
            "Epoch 11/40, training accuarcy: 0.8816, validation accuarcy: 0.85732421875\n",
            "Epoch 11/40, training loss: 0.37446947879852005, validation loss: 0.5110911726951599\n",
            "\t        recall: 0.6817\n",
            "\t      accuracy: 0.8573\n",
            "Epoch 12/40, training accuarcy: 0.8979, validation accuarcy: 0.86396484375\n",
            "Epoch 12/40, training loss: 0.3332652313883897, validation loss: 0.48375898599624634\n",
            "\t        recall: 0.7022\n",
            "\t      accuracy: 0.8640\n",
            "Epoch 13/40, training accuarcy: 0.90845, validation accuarcy: 0.85595703125\n",
            "Epoch 13/40, training loss: 0.2937256829563979, validation loss: 0.5336867570877075\n",
            "\t        recall: 0.7020\n",
            "\t      accuracy: 0.8560\n",
            "Epoch 14/40, training accuarcy: 0.916825, validation accuarcy: 0.894921875\n",
            "Epoch 14/40, training loss: 0.27093435539181826, validation loss: 0.37183940410614014\n",
            "\t        recall: 0.7332\n",
            "\t      accuracy: 0.8949\n",
            "Epoch 15/40, training accuarcy: 0.923, validation accuarcy: 0.89169921875\n",
            "Epoch 15/40, training loss: 0.24420811112519283, validation loss: 0.4771632254123688\n",
            "\t        recall: 0.7136\n",
            "\t      accuracy: 0.8917\n",
            "Epoch 16/40, training accuarcy: 0.926775, validation accuarcy: 0.8994140625\n",
            "Epoch 16/40, training loss: 0.23473107729368148, validation loss: 0.4170193672180176\n",
            "\t        recall: 0.7280\n",
            "\t      accuracy: 0.8994\n",
            "Epoch 17/40, training accuarcy: 0.934225, validation accuarcy: 0.90634765625\n",
            "Epoch 17/40, training loss: 0.21594092401729267, validation loss: 0.39893433451652527\n",
            "\t        recall: 0.7468\n",
            "\t      accuracy: 0.9063\n",
            "Epoch 18/40, training accuarcy: 0.93875, validation accuarcy: 0.91416015625\n",
            "Epoch 18/40, training loss: 0.1941730672386801, validation loss: 0.36706700921058655\n",
            "\t        recall: 0.7628\n",
            "\t      accuracy: 0.9142\n",
            "Epoch 19/40, training accuarcy: 0.94545, validation accuarcy: 0.90341796875\n",
            "Epoch 19/40, training loss: 0.17952273985382858, validation loss: 0.4198991358280182\n",
            "\t        recall: 0.7652\n",
            "\t      accuracy: 0.9034\n",
            "Epoch 20/40, training accuarcy: 0.94845, validation accuarcy: 0.9138671875\n",
            "Epoch 20/40, training loss: 0.17221719724167683, validation loss: 0.4136424958705902\n",
            "\t        recall: 0.7742\n",
            "\t      accuracy: 0.9139\n",
            "Epoch 21/40, training accuarcy: 0.95305, validation accuarcy: 0.91005859375\n",
            "Epoch 21/40, training loss: 0.1546282324536591, validation loss: 0.39435943961143494\n",
            "\t        recall: 0.7969\n",
            "\t      accuracy: 0.9101\n",
            "Epoch 22/40, training accuarcy: 0.954475, validation accuarcy: 0.9220703125\n",
            "Epoch 22/40, training loss: 0.15111789622219504, validation loss: 0.364070862531662\n",
            "\t        recall: 0.7968\n",
            "\t      accuracy: 0.9221\n",
            "Epoch 23/40, training accuarcy: 0.959, validation accuarcy: 0.9171875\n",
            "Epoch 23/40, training loss: 0.13642918928318723, validation loss: 0.41984859108924866\n",
            "\t        recall: 0.8142\n",
            "\t      accuracy: 0.9172\n",
            "Epoch 24/40, training accuarcy: 0.959875, validation accuarcy: 0.92392578125\n",
            "Epoch 24/40, training loss: 0.13675160717907225, validation loss: 0.3749638497829437\n",
            "\t        recall: 0.8025\n",
            "\t      accuracy: 0.9239\n",
            "Epoch 25/40, training accuarcy: 0.963925, validation accuarcy: 0.9208984375\n",
            "Epoch 25/40, training loss: 0.12249659179786968, validation loss: 0.42274966835975647\n",
            "\t        recall: 0.8068\n",
            "\t      accuracy: 0.9209\n",
            "Epoch 26/40, training accuarcy: 0.967975, validation accuarcy: 0.9322265625\n",
            "Epoch 26/40, training loss: 0.10931311860965316, validation loss: 0.36770468950271606\n",
            "\t        recall: 0.8433\n",
            "\t      accuracy: 0.9322\n",
            "Epoch 27/40, training accuarcy: 0.971875, validation accuarcy: 0.92548828125\n",
            "Epoch 27/40, training loss: 0.09608372531024514, validation loss: 0.42851683497428894\n",
            "\t        recall: 0.8224\n",
            "\t      accuracy: 0.9255\n",
            "Epoch 28/40, training accuarcy: 0.9723, validation accuarcy: 0.9236328125\n",
            "Epoch 28/40, training loss: 0.09932254203802841, validation loss: 0.41824737191200256\n",
            "\t        recall: 0.8278\n",
            "\t      accuracy: 0.9236\n",
            "Epoch 29/40, training accuarcy: 0.970475, validation accuarcy: 0.92607421875\n",
            "Epoch 29/40, training loss: 0.10004795314210235, validation loss: 0.40425392985343933\n",
            "\t        recall: 0.8326\n",
            "\t      accuracy: 0.9261\n",
            "Epoch 30/40, training accuarcy: 0.972775, validation accuarcy: 0.915625\n",
            "Epoch 30/40, training loss: 0.09601114148119833, validation loss: 0.4715496599674225\n",
            "\t        recall: 0.8301\n",
            "\t      accuracy: 0.9156\n",
            "Epoch 31/40, training accuarcy: 0.97555, validation accuarcy: 0.930859375\n",
            "Epoch 31/40, training loss: 0.08561235559499188, validation loss: 0.46463894844055176\n",
            "\t        recall: 0.8393\n",
            "\t      accuracy: 0.9309\n",
            "Epoch 32/40, training accuarcy: 0.97695, validation accuarcy: 0.9119140625\n",
            "Epoch 32/40, training loss: 0.08380518266397297, validation loss: 0.4903542101383209\n",
            "\t        recall: 0.8170\n",
            "\t      accuracy: 0.9119\n",
            "Epoch 33/40, training accuarcy: 0.9752, validation accuarcy: 0.937109375\n",
            "Epoch 33/40, training loss: 0.08957801352308434, validation loss: 0.4050486087799072\n",
            "\t        recall: 0.8566\n",
            "\t      accuracy: 0.9371\n",
            "Epoch 34/40, training accuarcy: 0.9803, validation accuarcy: 0.92783203125\n",
            "Epoch 34/40, training loss: 0.07148753248368668, validation loss: 0.4108316898345947\n",
            "\t        recall: 0.8594\n",
            "\t      accuracy: 0.9278\n",
            "Epoch 35/40, training accuarcy: 0.98005, validation accuarcy: 0.9359375\n",
            "Epoch 35/40, training loss: 0.07222889190551582, validation loss: 0.44822269678115845\n",
            "\t        recall: 0.8360\n",
            "\t      accuracy: 0.9359\n",
            "Epoch 36/40, training accuarcy: 0.98135, validation accuarcy: 0.93173828125\n",
            "Epoch 36/40, training loss: 0.0661256207853176, validation loss: 0.4237520694732666\n",
            "\t        recall: 0.8461\n",
            "\t      accuracy: 0.9317\n",
            "Epoch 37/40, training accuarcy: 0.982075, validation accuarcy: 0.93642578125\n",
            "Epoch 37/40, training loss: 0.0634907699267196, validation loss: 0.42173027992248535\n",
            "\t        recall: 0.8462\n",
            "\t      accuracy: 0.9364\n",
            "Epoch 38/40, training accuarcy: 0.980625, validation accuarcy: 0.934765625\n",
            "Epoch 38/40, training loss: 0.07349546470792051, validation loss: 0.46914416551589966\n",
            "\t        recall: 0.8457\n",
            "\t      accuracy: 0.9348\n",
            "Epoch 39/40, training accuarcy: 0.982125, validation accuarcy: 0.91787109375\n",
            "Epoch 39/40, training loss: 0.06751715247133735, validation loss: 0.4897089898586273\n",
            "\t        recall: 0.8569\n",
            "\t      accuracy: 0.9179\n",
            "Epoch 40/40, training accuarcy: 0.9786, validation accuarcy: 0.91611328125\n",
            "Epoch 40/40, training loss: 0.08378967369912536, validation loss: 0.45638343691825867\n",
            "\t        recall: 0.8460\n",
            "\t      accuracy: 0.9161\n",
            "\n",
            "Training Time (in minutes) = 32.243796098232266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEWCAYAAAByqrw/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSoaYACAd1t",
        "colab_type": "code",
        "outputId": "4e4ba26e-08ac-4564-e2a4-7a9818c9fc45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# FOR MAKING PREDICTIONS ON TEST SET AND SAVING TO CSV\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import shutil\n",
        "from torch import nn, optim\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# number of times we iterate over the training set\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 672\n",
        "TRAIN_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_x'\n",
        "TEST_PATH = '/content/drive/My Drive/McGill/comp551/data/test_max_x'\n",
        "TARGETS_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_y.csv'\n",
        "THRESH = 240\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('torch.cuda.device_count(): {0}'.format(torch.cuda.device_count()))\n",
        "    print('torch.cuda.get_device_name: {0}'.format(torch.cuda.get_device_name(0)))\n",
        "    # load images as a numpy array\n",
        "    X_train = np.array(np.load(TRAIN_PATH, allow_pickle=True))\n",
        "    X_train = np.array([cv2.threshold(i, THRESH, 255, cv2.THRESH_BINARY)[1] for i in X_train])\n",
        "    X_train = X_train / 255.0\n",
        "\n",
        "    X_test = np.array(np.load(TEST_PATH, allow_pickle=True))\n",
        "    X_test = np.array([cv2.threshold(i, THRESH, 255, cv2.THRESH_BINARY)[1] for i in X_test])\n",
        "    X_test = X_test / 255.0\n",
        "\n",
        "    y_train = pd.read_csv(TARGETS_PATH, delimiter=',', skipinitialspace=True)\n",
        "    y_train = y_train.to_numpy()\n",
        "    # remove id column\n",
        "    y_train = y_train[:, 1]\n",
        "    y_train = y_train.astype(int)\n",
        "\n",
        "    # Clean memory\n",
        "    len_train_dataset = len(X_train)\n",
        "\n",
        "    X_train = torch.from_numpy(X_train)\n",
        "    y_train = torch.from_numpy(y_train)\n",
        "    X_test = torch.from_numpy(X_test)\n",
        "    y_train = y_train.long()\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=BATCH_SIZE, num_workers=8)\n",
        "\n",
        "    # defining the model\n",
        "    model = Net().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    time0 = time()\n",
        "\n",
        "    for e in range(EPOCHS):\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # clearing the Gradients of the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prediction for training\n",
        "        output_train = model(inputs)\n",
        "        \n",
        "        # computing the training loss\n",
        "        loss_train = criterion(output_train, labels)\n",
        "\n",
        "        # computing the updated weights of all the model parameters\n",
        "        loss_train.backward()\n",
        "\n",
        "        # And optimizes its weights here\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_train.item()\n",
        "      else:\n",
        "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss / len_train_dataset))\n",
        "\n",
        "    print(\"\\nTraining Time (in minutes) =\", (time() - time0) / 60)\n",
        "\n",
        "    # prediction for test set\n",
        "    predictions = np.array([])\n",
        "    with torch.no_grad():\n",
        "      for images in test_loader:\n",
        "        images = images.to(device)\n",
        "        output = model(images)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        predictions = np.append(predictions, predicted.cpu().numpy())\n",
        "\n",
        "    predictions = predictions.astype(int)\n",
        "    Id = list(range(0, 10000))\n",
        "    Id = np.asarray(Id)\n",
        "    results = pd.DataFrame({'Id': Id, 'Label': predictions}, columns=['Id', 'Label'])\n",
        "    from google.colab import files\n",
        "    results.to_csv('predictions.csv', index=False)\n",
        "    shutil.move(\"/content/predictions.csv\", \"/content/drive/My Drive/McGill/comp551/predictions.csv\")\n",
        "    print('Results have been saved to predictions.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.cuda.device_count(): 1\n",
            "torch.cuda.get_device_name: Tesla P100-PCIE-16GB\n",
            "Epoch 0 - Training loss: 0.0032704003858566285\n",
            "Epoch 1 - Training loss: 0.0029089065623283386\n",
            "Epoch 2 - Training loss: 0.002827518105506897\n",
            "Epoch 3 - Training loss: 0.0024329508543014526\n",
            "Epoch 4 - Training loss: 0.002178996772766113\n",
            "Epoch 5 - Training loss: 0.0019570931363105774\n",
            "Epoch 6 - Training loss: 0.0015498412847518921\n",
            "Epoch 7 - Training loss: 0.0011921281003952025\n",
            "Epoch 8 - Training loss: 0.0009742726159095764\n",
            "Epoch 9 - Training loss: 0.000797192497253418\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}