{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_88.62_Anton",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nJ47ABzStdv",
        "colab_type": "code",
        "outputId": "e323e7bb-d279-47eb-980e-6424f9322680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hWJmOgwzCWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Net, self).__init__()\n",
        "        # Defining a 2D convolution layer\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1),\n",
        "            # nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            # nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            # nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "        self.drop_out = nn.Dropout()\n",
        "\n",
        "        # Defining another 2D convolution layer\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(6*6*128, 512)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-TGbFWTQ49",
        "colab_type": "code",
        "outputId": "4dec60b3-fec7-4bbc-f14a-d0805174c26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from torch import nn, optim\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# number of times we iterate over the training set\n",
        "EPOCH = 70\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 512\n",
        "TRAIN_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_x'\n",
        "TEST_PATH = ''\n",
        "TARGETS_PATH = '/content/drive/My Drive/McGill/comp551/data/train_max_y.csv'\n",
        "THRESH = 240\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('torch.cuda.device_count(): {0}'.format(torch.cuda.device_count()))\n",
        "    print('torch.cuda.get_device_name: {0}'.format(torch.cuda.get_device_name(0)))\n",
        "    # load images as a numpy array\n",
        "    train_dataset = np.array(np.load(TRAIN_PATH, allow_pickle=True))\n",
        "    train_dataset = np.array([cv2.threshold(i, THRESH, 255, cv2.THRESH_BINARY)[1] for i in train_dataset])\n",
        "    train_dataset = train_dataset / 255.0\n",
        "\n",
        "    targets = pd.read_csv(TARGETS_PATH, delimiter=',', skipinitialspace=True)\n",
        "    targets = targets.to_numpy()\n",
        "    # remove id column\n",
        "    targets = targets[:, 1]\n",
        "    targets = targets.astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(train_dataset, targets, test_size=0.2, random_state=42)\n",
        "    # Clean memory\n",
        "    len_train_dataset = len(X_train)\n",
        "    train_dataset = None\n",
        "    targets = None\n",
        "\n",
        "    X_train = torch.from_numpy(X_train)\n",
        "    y_train = torch.from_numpy(y_train)\n",
        "    X_test = torch.from_numpy(X_test)\n",
        "    y_test = torch.from_numpy(y_test)\n",
        "    y_train = y_train.long()\n",
        "    y_test = y_test.long()\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test,y_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
        "\n",
        "    # defining the model\n",
        "    model = Net().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    time0 = time()\n",
        "    epochs = EPOCH\n",
        "\n",
        "    for e in range(epochs):\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # clearing the Gradients of the model parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prediction for training\n",
        "        output_train = model(inputs)\n",
        "        \n",
        "        # computing the training loss\n",
        "        loss_train = criterion(output_train, labels)\n",
        "\n",
        "        # computing the updated weights of all the model parameters\n",
        "        loss_train.backward()\n",
        "\n",
        "        # And optimizes its weights here\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_train.item()\n",
        "      else:\n",
        "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss / len_train_dataset))\n",
        "\n",
        "    print(\"\\nTraining Time (in minutes) =\", (time() - time0) / 60)\n",
        "\n",
        "    # prediction for validation set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            output = model(images)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # accuracy on validation set\n",
        "    print(\"\\nModel Accuracy =\", (100 * correct / total))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.cuda.device_count(): 1\n",
            "torch.cuda.get_device_name: Tesla K80\n",
            "Epoch 0 - Training loss: 0.0038247930645942687\n",
            "Epoch 1 - Training loss: 0.0037175198674201967\n",
            "Epoch 2 - Training loss: 0.0033434190273284913\n",
            "Epoch 3 - Training loss: 0.002359899887442589\n",
            "Epoch 4 - Training loss: 0.001668053065240383\n",
            "Epoch 5 - Training loss: 0.0013070116579532623\n",
            "Epoch 6 - Training loss: 0.0010887453399598599\n",
            "Epoch 7 - Training loss: 0.0009471268929541111\n",
            "Epoch 8 - Training loss: 0.0008488050632178783\n",
            "Epoch 9 - Training loss: 0.000755842249840498\n",
            "Epoch 10 - Training loss: 0.0006905513152480126\n",
            "Epoch 11 - Training loss: 0.0006460615105926991\n",
            "Epoch 12 - Training loss: 0.000623036963865161\n",
            "Epoch 13 - Training loss: 0.0005733240639790892\n",
            "Epoch 14 - Training loss: 0.0005434309773147106\n",
            "Epoch 15 - Training loss: 0.0005036316730082035\n",
            "Epoch 16 - Training loss: 0.0004766526564955711\n",
            "Epoch 17 - Training loss: 0.00043377025574445723\n",
            "Epoch 18 - Training loss: 0.0004029659021645784\n",
            "Epoch 19 - Training loss: 0.0003880924915894866\n",
            "Epoch 20 - Training loss: 0.00039272366277873515\n",
            "Epoch 21 - Training loss: 0.0003816743716597557\n",
            "Epoch 22 - Training loss: 0.00040858808904886245\n",
            "Epoch 23 - Training loss: 0.0003954742399044335\n",
            "Epoch 24 - Training loss: 0.000340141336992383\n",
            "Epoch 25 - Training loss: 0.0003227587340399623\n",
            "Epoch 26 - Training loss: 0.0003076593618839979\n",
            "Epoch 27 - Training loss: 0.00029486620966345074\n",
            "Epoch 28 - Training loss: 0.00030436330176889897\n",
            "Epoch 29 - Training loss: 0.00025046780221164226\n",
            "Epoch 30 - Training loss: 0.00024554192312061787\n",
            "Epoch 31 - Training loss: 0.0002294504089280963\n",
            "Epoch 32 - Training loss: 0.00023372610695660115\n",
            "Epoch 33 - Training loss: 0.00022261679880321026\n",
            "Epoch 34 - Training loss: 0.00022167301895096897\n",
            "Epoch 35 - Training loss: 0.0002229641747660935\n",
            "Epoch 36 - Training loss: 0.00021430562995374203\n",
            "Epoch 37 - Training loss: 0.00021367194280028342\n",
            "Epoch 38 - Training loss: 0.00021499105812981725\n",
            "Epoch 39 - Training loss: 0.0001969347652979195\n",
            "Epoch 40 - Training loss: 0.0002021700785495341\n",
            "Epoch 41 - Training loss: 0.00021430362556129693\n",
            "Epoch 42 - Training loss: 0.00020831530932337045\n",
            "Epoch 43 - Training loss: 0.00020644922591745853\n",
            "Epoch 44 - Training loss: 0.0002255001432262361\n",
            "Epoch 45 - Training loss: 0.00020410166308283806\n",
            "Epoch 46 - Training loss: 0.00018082551173865795\n",
            "Epoch 47 - Training loss: 0.00018813662249594928\n",
            "Epoch 48 - Training loss: 0.0001832142279483378\n",
            "Epoch 49 - Training loss: 0.000173124566860497\n",
            "Epoch 50 - Training loss: 0.00016715142764151097\n",
            "Epoch 51 - Training loss: 0.00017204524385742843\n",
            "Epoch 52 - Training loss: 0.00016105931997299194\n",
            "Epoch 53 - Training loss: 0.00016338939554989338\n",
            "Epoch 54 - Training loss: 0.00016226590611040592\n",
            "Epoch 55 - Training loss: 0.00016008901791647078\n",
            "Epoch 56 - Training loss: 0.0001500487326644361\n",
            "Epoch 57 - Training loss: 0.00015967074329964817\n",
            "Epoch 58 - Training loss: 0.00014584016781300308\n",
            "Epoch 59 - Training loss: 0.00015994476359337568\n",
            "Epoch 60 - Training loss: 0.00014902181169018148\n",
            "Epoch 61 - Training loss: 0.0001414063506759703\n",
            "Epoch 62 - Training loss: 0.00014707002127543092\n",
            "Epoch 63 - Training loss: 0.0001519398445263505\n",
            "Epoch 64 - Training loss: 0.00013999331016093493\n",
            "Epoch 65 - Training loss: 0.00015706611201167107\n",
            "Epoch 66 - Training loss: 0.0001563267226330936\n",
            "Epoch 67 - Training loss: 0.00013985933735966683\n",
            "Epoch 68 - Training loss: 0.00016592849353328347\n",
            "Epoch 69 - Training loss: 0.00014626401914283633\n",
            "\n",
            "Training Time (in minutes) = 110.23441347281138\n",
            "\n",
            "Model Accuracy = 90.16\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}